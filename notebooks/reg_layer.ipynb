{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: org.freedesktop.DBus.Error.FileNotFound: Failed to connect to socket /run/user/1409/bus: No such file or directory\n",
      "Requirement already satisfied: tensorflow_graphics in /home/ltetrel/.local/lib/python3.6/site-packages (1.0.0)\n",
      "Requirement already satisfied: tensorflow-probability in /home/ltetrel/.local/lib/python3.6/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (1.18.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (1.4.1)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (0.9.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_graphics) (1.14.0)\n",
      "Requirement already satisfied: tensorflow>=1.13.1 in /home/ltetrel/.local/lib/python3.6/site-packages (from tensorflow_graphics) (2.1.0)\n",
      "Requirement already satisfied: gast>=0.2 in /home/ltetrel/.local/lib/python3.6/site-packages (from tensorflow-probability) (0.2.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.2 in /home/ltetrel/.local/lib/python3.6/site-packages (from tensorflow-probability) (1.3.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability) (4.4.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ltetrel/.local/lib/python3.6/site-packages (from tensorflow>=1.13.1->tensorflow_graphics) (0.8.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow_graphics) (3.11.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow_graphics) (1.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ltetrel/.local/lib/python3.6/site-packages (from tensorflow>=1.13.1->tensorflow_graphics) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ltetrel/.local/lib/python3.6/site-packages (from tensorflow>=1.13.1->tensorflow_graphics) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ltetrel/.local/lib/python3.6/site-packages (from tensorflow>=1.13.1->tensorflow_graphics) (0.33.6)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /home/ltetrel/.local/lib/python3.6/site-packages (from tensorflow>=1.13.1->tensorflow_graphics) (2.1.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /home/ltetrel/.local/lib/python3.6/site-packages (from tensorflow>=1.13.1->tensorflow_graphics) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow_graphics) (1.28.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow_graphics) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.13.1->tensorflow_graphics) (3.2.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/ltetrel/.local/lib/python3.6/site-packages (from tensorflow>=1.13.1->tensorflow_graphics) (1.0.8)\n",
      "Requirement already satisfied: setuptools in /home/ltetrel/.local/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow>=1.13.1->tensorflow_graphics) (41.6.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ltetrel/.local/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (0.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (2.23.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (1.13.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (3.2.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (0.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.13.1->tensorflow_graphics) (2.10.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (2020.4.5.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (4.1.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=1.13.1->tensorflow_graphics) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append('/home/ltetrel/DeepNeuroAN/deepneuroan/')\n",
    "sys.path.append('/home/ltetrel/DeepNeuroAN/')\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "import tensorflow as tf\n",
    "\n",
    "from preproc import create_ref_grid\n",
    "import deepneuroan.utils as utils\n",
    "import SimpleITK as sitk\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "!python3 -m pip install --user tensorflow_graphics tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs creation   \n",
    "batch_size = 10\n",
    "x = np.empty((batch_size, 9, 9, 9, 1), dtype=np.float64)\n",
    "trf = np.empty((batch_size, 7), dtype=np.float64)\n",
    "data_dir = \"./data\"\n",
    "\n",
    "for i in range(batch_size):\n",
    "    x[i, :, :, :, 0] = sitk.GetArrayFromImage(sitk.ReadImage(data_dir + \"/ses-vid001_task-video_run-01_bold_vol-0001_transfo-%06d.nii.gz\" %(i+1)\n",
    "                                   , sitk.sitkFloat64))[100:109, 110:119, 120:129]\n",
    "\n",
    "    trf[i,] = utils.load_trf_file(data_dir + \"/ses-vid001_task-video_run-01_bold_vol-0001_transfo-%06d.txt\" %(i+1))\n",
    "    \n",
    "    # Inversing quaternions to compare volumes with base one\n",
    "    q = sitk.VersorRigid3DTransform([trf[i, 1], trf[i, 2], trf[i, 3], trf[i, 0]])\n",
    "    t = sitk.TranslationTransform(3, tuple(trf[i, 4:]))\n",
    "    q.SetTranslation(t.GetOffset())\n",
    "    q = q.GetInverse().GetParameters()\n",
    "    trf[i, 1:4] = [-trf[i, 1], -trf[i, 2], -trf[i, 3]]\n",
    "    trf[i, 4:] = q[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTransformation(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.trainable = False\n",
    "        super(self.__class__, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        if len(input_shape) > 2:\n",
    "            raise Exception(\"LinearRegistration must be called on a list of length 2. \"\n",
    "                            \"First argument is the volume, second is the quaternion transform.\")\n",
    "\n",
    "        super(self.__class__, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        assert isinstance(inputs, list)\n",
    "        \n",
    "        #https://www.tensorflow.org/graphics/api_docs/python/tfg/math/interpolation/bspline/interpolate\n",
    "        #https://www.tensorflow.org/probability/api_docs/python/tfp/math/batch_interp_regular_nd_grid\n",
    "        #https://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/layers/spatial_transformer.html\n",
    "        \n",
    "        # map transform across batch\n",
    "        return tf.map_fn(self._tf_single_transform, inputs, dtype=tf.float32)\n",
    "            \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]   \n",
    "    \n",
    "    def _single_transform(self, src, trf):\n",
    "        ref_grid = create_ref_grid()\n",
    "\n",
    "        sitk_src = utils.get_sitk_from_numpy(src, ref_grid)\n",
    "        sitk_tgt = utils.transform_volume(sitk_src, ref_grid, rigid=trf)\n",
    "\n",
    "        return sitk.GetArrayFromImage(sitk_tgt)\n",
    "    \n",
    "    @tf.function\n",
    "    def _tf_single_transform(self, inputs): \n",
    "        out = tf.numpy_function(self._single_transform, inp=[inputs[0], inputs[1]], Tout=tf.float32) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: When it is working, uncomment\n",
    "\n",
    "# n_batch = 7\n",
    "# transfos = tf.random.uniform(shape=(n_batch, 7), seed=0) #quaternions (4,) + translations (3,) + scales (3,)\n",
    "# U = tf.random.uniform((n_batch, 220, 220, 220, 1))\n",
    "# out_size = [10, 10, 10]\n",
    "# name='BatchSpatialTransformer3dAffine'\n",
    "\n",
    "# ref_grid = create_ref_grid()\n",
    "# sz_ref = ref_grid.GetSize()\n",
    "# min_ref_grid = ref_grid.GetOrigin()\n",
    "# max_ref_grid = ref_grid.TransformIndexToPhysicalPoint(sz_ref)\n",
    "\n",
    "# with tf.compat.v1.variable_scope(name):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired from https://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/layers/spatial_transformer.html\n",
    "def _repeat(x, n_repeats):\n",
    "    rep = tf.transpose(a=tf.expand_dims(tf.ones(\n",
    "                                                shape=tf.stack([n_repeats,])\n",
    "                                                ), 1)\n",
    "                       , perm=[1, 0])\n",
    "    rep = tf.cast(rep, dtype=tf.int32)\n",
    "    x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n",
    "    return tf.reshape(x, [-1])\n",
    "\n",
    "def _interpolate(im, p, out_size, min_ref_grid, max_ref_grid):\n",
    "\n",
    "    # getting position coords as arrays\n",
    "    x = tf.reshape(tf.slice(p, [0, 0, 0], [-1, 1, -1]), [-1])\n",
    "    y = tf.reshape(tf.slice(p, [0, 1, 0], [-1, 1, -1]), [-1])\n",
    "    z = tf.reshape(tf.slice(p, [0, 2, 0], [-1, 1, -1]), [-1])\n",
    "\n",
    "    # constants\n",
    "    num_batch = tf.shape(im)[0]\n",
    "    height = tf.shape(im)[1]\n",
    "    width = tf.shape(im)[2]\n",
    "    depth = tf.shape(im)[3]\n",
    "    channels = tf.shape(im)[4]\n",
    "    out_height = out_size[0]\n",
    "    out_width = out_size[1]\n",
    "    out_depth = out_size[2]\n",
    "    height_f = tf.cast(height, dtype=tf.float32)\n",
    "    width_f = tf.cast(width, dtype=tf.float32)\n",
    "    depth_f = tf.cast(depth, dtype=tf.float32)\n",
    "    zero = tf.zeros([], dtype=tf.float32)\n",
    "\n",
    "    # scale positions to [0, width/height - 1]\n",
    "    ix = (x - min_ref_grid[0]) * (width_f - 1.)/(max_ref_grid[0] - min_ref_grid[0])\n",
    "    iy = (y - min_ref_grid[1]) * (height_f - 1.)/(max_ref_grid[1] - min_ref_grid[1])\n",
    "    iz = (z - min_ref_grid[2]) * (depth_f - 1.)/(max_ref_grid[2] - min_ref_grid[2])\n",
    "\n",
    "    # border padding mode, for positions outside of refrence grid\n",
    "    ix = tf.clip_by_value(ix, zero, width_f - 1)\n",
    "    iy = tf.clip_by_value(iy, zero, height_f - 1)\n",
    "    iz = tf.clip_by_value(iz, zero, depth_f - 1)\n",
    "\n",
    "    # get corner indexes based on the scaled positions\n",
    "    # nwt stands for north-west-top; seb stands for south-east-bottom\n",
    "    ix_nwt = tf.floor(ix)\n",
    "    iy_nwt = tf.floor(iy)\n",
    "    iz_nwt = tf.floor(iz)\n",
    "    ix_net = ix_nwt + 1\n",
    "    iy_net = iy_nwt\n",
    "    iz_net = iz_nwt\n",
    "    ix_swt = ix_nwt\n",
    "    iy_swt = iy_nwt + 1\n",
    "    iz_swt = iz_nwt\n",
    "    ix_set = ix_nwt + 1\n",
    "    iy_set = iy_nwt + 1\n",
    "    iz_set = iz_nwt\n",
    "    ix_nwb = ix_nwt\n",
    "    iy_nwb = iy_nwt\n",
    "    iz_nwb = iz_nwt + 1\n",
    "    ix_neb = ix_nwt + 1\n",
    "    iy_neb = iy_nwt\n",
    "    iz_neb = iz_nwt + 1\n",
    "    ix_swb = ix_nwt\n",
    "    iy_swb = iy_nwt + 1\n",
    "    iz_swb = iz_nwt + 1\n",
    "    ix_seb = ix_nwt + 1\n",
    "    iy_seb = iy_nwt + 1\n",
    "    iz_seb = iz_nwt + 1\n",
    "\n",
    "    # calculate the weights for each p position\n",
    "    nwt = tf.expand_dims(((ix_seb - ix)    * (iy_seb - iy)    * (iz_seb - iz)), 1)\n",
    "    net = tf.expand_dims(((ix    - ix_swb) * (iy_swb - iy)    * (iz_swb - iz)), 1)\n",
    "    swt = tf.expand_dims(((ix_neb - ix)    * (iy    - iy_neb) * (iz_neb - iz)), 1)\n",
    "    set = tf.expand_dims(((ix    - ix_nwb) * (iy    - iy_nwb) * (iz_nwb - iz)), 1)\n",
    "    nwb = tf.expand_dims(((ix_set - ix)    * (iy_set - iy)    * (iz - iz_set)), 1)\n",
    "    neb = tf.expand_dims(((ix    - ix_swt) * (iy_swt - iy)    * (iz - iz_swt)), 1)\n",
    "    swb = tf.expand_dims(((ix_net - ix)    * (iy    - iy_net) * (iz - iz_net)), 1)\n",
    "    seb = tf.expand_dims(((ix    - ix_nwt) * (iy    - iy_nwt) * (iz - iz_nwt)), 1)\n",
    "\n",
    "    # gather input img values from positions\n",
    "    # get corners idx\n",
    "    ibatch = _repeat(tf.range(num_batch) * height * width * depth, out_height * out_width * out_depth)\n",
    "    idx_nwt = ibatch + tf.cast(tf.clip_by_value(ix_nwt, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_nwt, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_nwt, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_net = ibatch + tf.cast(tf.clip_by_value(ix_net, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_net, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_net, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_swt = ibatch + tf.cast(tf.clip_by_value(ix_swt, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_swt, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_swt, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_set = ibatch + tf.cast(tf.clip_by_value(ix_set, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_set, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_set, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_nwb = ibatch + tf.cast(tf.clip_by_value(ix_nwb, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_nwb, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_nwb, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_neb = ibatch + tf.cast(tf.clip_by_value(ix_neb, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_neb, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_neb, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_swb = ibatch + tf.cast(tf.clip_by_value(ix_swb, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_swb, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_swb, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_seb = ibatch + tf.cast(tf.clip_by_value(ix_seb, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_seb, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_seb, zero, depth_f - 1), tf.int32) * width * height\n",
    " \n",
    "    # gather input image values from idx\n",
    "    im_flat = tf.reshape(im, tf.stack([-1, channels]))\n",
    "    I_nwt = tf.gather(im_flat, idx_nwt)\n",
    "    I_net = tf.gather(im_flat, idx_net)\n",
    "    I_swt = tf.gather(im_flat, idx_swt)\n",
    "    I_set = tf.gather(im_flat, idx_set)\n",
    "    I_nwb = tf.gather(im_flat, idx_nwb)\n",
    "    I_neb = tf.gather(im_flat, idx_neb)\n",
    "    I_swb = tf.gather(im_flat, idx_swb)\n",
    "    I_seb = tf.gather(im_flat, idx_seb)\n",
    "    \n",
    "    output = tf.add_n([nwt * I_nwt, net * I_net, swt * I_swt, set * I_set, nwb * I_nwb, neb * I_neb, swb * I_swb, seb * I_seb])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x = tf.constant([[5., 5., 5.], [5., 5., 5.]])\n",
    "y = tf.constant([[5., 7., 10.], [5., 5., 6.]])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1000):\n",
    "    y + (x-y)*tf.cast(tf.math.less(x, y), dtype=tf.float32)\n",
    "ElpsTime = time.time() - tic\n",
    "print(\"*** Total %1.3f s ***\"%(ElpsTime))\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1000):\n",
    "    tf.math.minimum(5., y)\n",
    "ElpsTime = time.time() - tic\n",
    "print(\"*** Total %1.3f s ***\"%(ElpsTime))\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1000):\n",
    "    tf.clip_by_value(y, 0., 5.)\n",
    "ElpsTime = time.time() - tic\n",
    "print(\"*** Total %1.3f s ***\"%(ElpsTime))\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1000):\n",
    "    tf.clip_by_value(y, 0., 5.)\n",
    "ElpsTime = time.time() - tic\n",
    "print(\"*** Total %1.3f s ***\"%(ElpsTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = tf.random.uniform(shape=(220*220*220,), minval=-50, maxval=50, seed=0, dtype=tf.float32)\n",
    "iy = tf.random.uniform(shape=(220*220*220,), minval=-50, maxval=50, seed=0, dtype=tf.float32)\n",
    "iz = tf.random.uniform(shape=(220*220*220,), minval=-50, maxval=50, seed=0, dtype=tf.float32)\n",
    "\n",
    "\n",
    "I = tf.random.uniform(shape=(220*220*220, 1))\n",
    "\n",
    "import time\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "    ix_t = tf.clip_by_value(ix, 0., 10.)\n",
    "    iy_t = tf.clip_by_value(iy, 0., 20.)\n",
    "    iz_t = tf.clip_by_value(iz, 0., 30.)\n",
    "\n",
    "    #     get corner indexes based on the scaled positions\n",
    "    #     nwt stands for north-west-top; seb stands for south-east-bottom\n",
    "    ix_nwt = tf.floor(ix_t)\n",
    "    iy_nwt = tf.floor(iy_t)\n",
    "    iz_nwt = tf.floor(iz_t)\n",
    "    ix_net = ix_nwt + 1\n",
    "    iy_net = iy_nwt\n",
    "    iz_net = iz_nwt\n",
    "    ix_swt = ix_nwt\n",
    "    iy_swt = iy_nwt + 1\n",
    "    iz_swt = iz_nwt\n",
    "    ix_set = ix_nwt + 1\n",
    "    iy_set = iy_nwt + 1\n",
    "    iz_set = iz_nwt\n",
    "    ix_nwb = ix_nwt\n",
    "    iy_nwb = iy_nwt\n",
    "    iz_nwb = iz_nwt + 1\n",
    "    ix_neb = ix_nwt + 1\n",
    "    iy_neb = iy_nwt\n",
    "    iz_neb = iz_nwt + 1\n",
    "    ix_swb = ix_nwt\n",
    "    iy_swb = iy_nwt + 1\n",
    "    iz_swb = iz_nwt + 1\n",
    "    ix_seb = ix_nwt + 1\n",
    "    iy_seb = iy_nwt + 1\n",
    "    iz_seb = iz_nwt + 1\n",
    "\n",
    "    nwt = tf.expand_dims(((ix_seb - ix)    * (iy_seb - iy)    * (iz_seb - iz)), 1)\n",
    "    net = tf.expand_dims(((ix    - ix_swb) * (iy_swb - iy)    * (iz_swb - iz)), 1)\n",
    "    swt = tf.expand_dims(((ix_neb - ix)    * (iy    - iy_neb) * (iz_neb - iz)), 1)\n",
    "    set = tf.expand_dims(((ix    - ix_nwb) * (iy    - iy_nwb) * (iz_nwb - iz)), 1)\n",
    "    nwb = tf.expand_dims(((ix_set - ix)    * (iy_set - iy)    * (iz - iz_set)), 1)\n",
    "    neb = tf.expand_dims(((ix    - ix_swt) * (iy_swt - iy)    * (iz - iz_swt)), 1)\n",
    "    swb = tf.expand_dims(((ix_net - ix)    * (iy    - iy_net) * (iz - iz_net)), 1)\n",
    "    seb = tf.expand_dims(((ix    - ix_nwt) * (iy    - iy_nwt) * (iz - iz_nwt)), 1)\n",
    "\n",
    "    tf.add_n([nwt * I, net * I, swt * I, set * I, nwb * I, neb * I, swb * I, seb * I])\n",
    "\n",
    "\n",
    "ElpsTime = time.time() - tic\n",
    "print(\"*** Total %1.3f s ***\"%(ElpsTime))\n",
    "\n",
    "I = tf.random.uniform(shape=(1, 220*220*220))\n",
    "tic = time.time()\n",
    "for i in range(10):\n",
    "#     ip = tf.stack( [tf.clip_by_value(ix, 0., 10.)\n",
    "#                 , tf.clip_by_value(iy, 0., 20.)\n",
    "#                 , tf.clip_by_value(iz, 0., 30.)], axis=0)\n",
    "#     ix = tf.floor(ix)\n",
    "#     iy = tf.floor(iy)\n",
    "#     iz = tf.floor(iz)\n",
    "#     corners = tf.stack( [tf.stack([ix, iy, iz], axis=0),\n",
    "#                          tf.stack([ix + 1., iy, iz], axis=0),\n",
    "#                          tf.stack([ix, iy + 1., iz], axis=0),\n",
    "#                          tf.stack([ix + 1., iy + 1., iz], axis=0),\n",
    "#                          tf.stack([ix, iy, iz + 1.], axis=0),\n",
    "#                          tf.stack([ix + 1., iy, iz + 1.], axis=0),\n",
    "#                          tf.stack([ix, iy + 1., iz + 1.], axis=0),\n",
    "#                          tf.stack([ix + 1., iy + 1., iz + 1.], axis=0)], axis=1)\n",
    "#     corners = tf.transpose(corners, perm=(1, 0, 2))\n",
    "#     N = tf.math.reduce_prod(tf.abs(corners - ip), axis=1)\n",
    "    \n",
    "#     tf.linalg.matmul(I, N)\n",
    "    \n",
    "    tf.add_n([nwt, net, swt, set, nwb, neb, swb, seb]) * I\n",
    "\n",
    "    \n",
    "ElpsTime = time.time() - tic\n",
    "print(\"*** Total %1.3f s ***\"%(ElpsTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "min_f = 10.\n",
    "min_d = 10\n",
    "ix = tf.random.uniform(shape=(10*220*220*220,), minval=-50, maxval=50, seed=0, dtype=tf.float32)\n",
    "ix_d = tf.random.uniform(shape=(10*220*220*220,), minval=-50, maxval=50, seed=0, dtype=tf.int32)\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(100):\n",
    "    ix + 1.\n",
    "ElpsTime = time.time() - tic\n",
    "print(\"*** Total %1.3f s ***\"%(ElpsTime))\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(100):\n",
    "    ix_d + 1\n",
    "ElpsTime = time.time() - tic\n",
    "print(\"*** Total %1.3f s ***\"%(ElpsTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cast([-3.0 -0.5, -0.6, -0.4, 1.2], tf.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 7\n",
    "print(tf.math.reduce_prod(tf.cast(corners[0, b, :] == ix_seb, tf.float32)))\n",
    "print(tf.math.reduce_prod(tf.cast(corners[1, b, :] == iy_seb, tf.float32)))\n",
    "print(tf.math.reduce_prod(tf.cast(corners[2, b, :] == iz_seb, tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.where(corners[0, b, :] == ix_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 3\n",
    "transfos = tf.random.uniform(shape=(n_batch, 7), seed=0, dtype=tf.float32) #quaternions (4,) + translations (3,) + scales (3,)\n",
    "transfos = tf.stack( [0.*tf.ones(shape=(7), dtype=tf.float32), (-10)*tf.ones(shape=(7), dtype=tf.float32), (-50)*tf.ones(shape=(7), dtype=tf.float32)] )\n",
    "U_single = 34*tf.random.uniform((1, 220, 220, 220, 1), seed=0, dtype=tf.float32)\n",
    "\n",
    "np.random.seed(seed=0)\n",
    "U_single = 34*np.random.rand(1, 220, 220, 220, 1)\n",
    "U_single[:, 20:25, :, :, :] = 255.\n",
    "U_single[:, :, 20:25, :, :] = 255.\n",
    "U_single[:, :, :, 20:25, :] = 255.\n",
    "U_single[:, 195:200, :, :, :] = 255.\n",
    "U_single[:, :, 195:200, :, :] = 255.\n",
    "U_single[:, :, :, 195:200, :] = 255.\n",
    "U_single[:, :20, :, :, :] = 0.\n",
    "U_single[:, :, :20, :, :] = 0.\n",
    "U_single[:, :20, :, :, :] = 0.\n",
    "U_single[:, 200:, :, :, :] = 0.\n",
    "U_single[:, :, 200:, :, :] = 0.\n",
    "U_single[:, :, :, 200:, :] = 0.\n",
    "U = tf.tile( tf.cast(U_single, dtype=tf.float32), (n_batch, 1, 1, 1, 1), name=None)\n",
    "\n",
    "out_size = [220, 220, 220]\n",
    "name='BatchSpatialTransformer3dAffine'\n",
    "\n",
    "ref_size = tf.shape(U)[1:-1]\n",
    "# TODO: min[d] and max[d] correspond to cartesian coordinate d (d=0 is x, d=1 is y ..)\n",
    "# min_ref_grid = tf.constant([-30., -10., 15.])\n",
    "# max_ref_grid = tf.constant([-18., 10., 37])\n",
    "min_ref_grid = tf.constant([0., 0., 0.], dtype=tf.float32)\n",
    "max_ref_grid = tf.constant(tf.stack(tf.cast(ref_size, dtype=tf.float32) - 1), dtype=tf.float32)\n",
    "\n",
    "with tf.compat.v1.variable_scope(name):\n",
    "    input_dim = U\n",
    "    \n",
    "    num_batch = tf.shape(input=input_dim)[0]\n",
    "    num_channels = tf.shape(input=input_dim)[-1]\n",
    "    \n",
    "    #if the transformations has length > 7, then we apply scaling\n",
    "    if tf.shape(transfos)[-1] > 7:\n",
    "        thetas = tf.linalg.diag(transfos[:, -3:])\n",
    "    else:\n",
    "        thetas = tf.eye(num_rows=3, batch_shape=[num_batch])\n",
    "        \n",
    "    #if the transformations has length > 4, then we apply translation\n",
    "    if tf.shape(transfos)[-1] > 4:  \n",
    "        thetas = tf.concat(axis=2, values=[thetas, transfos[:, 4:7, tf.newaxis]])\n",
    "    else:\n",
    "        thetas = tf.concat(axis=2, values=[thetas, tf.zeros((num_batch, 3, 1))])\n",
    "    \n",
    "    # physical points of source volume, from template affine\n",
    "    # if we don't have volume affine, we use a grid [-1, 1]\n",
    "    if (min_ref_grid is None) | (max_ref_grid is None):\n",
    "        min_ref_grid = (-1)*tf.ones(3)\n",
    "        max_ref_grid = tf.ones(3)\n",
    "    m_y, m_z, m_x = tf.meshgrid(tf.linspace(min_ref_grid[1], max_ref_grid[1], ref_size[0]),\n",
    "                                tf.linspace(min_ref_grid[2], max_ref_grid[2], ref_size[2]),\n",
    "                                tf.linspace(min_ref_grid[0], max_ref_grid[0], ref_size[1]),\n",
    "                                indexing='xy')\n",
    "    \n",
    "    # physical points for quaternion\n",
    "    grid = tf.concat(axis=0, values=[tf.reshape(m_x, (1, -1)), tf.reshape(m_y, (1, -1)), tf.reshape(m_z, (1, -1))])\n",
    "    grid = tf.transpose(grid)\n",
    "    print(\"quaternion rotate here\")\n",
    "#     grid = tfg.geometry.transformation.quaternion.rotate(grid, transfos[:, :4])\n",
    "\n",
    "    grid = tf.transpose(grid)\n",
    "    grid = tf.concat(axis=0, values=[grid, tf.ones_like(tf.reshape(m_x, (1, -1)))])\n",
    "    # adding batch dimension\n",
    "    grid = tf.expand_dims(grid, 0)\n",
    "    grid = tf.reshape(grid, [-1])\n",
    "    grid = tf.tile(grid, tf.stack([num_batch]))\n",
    "    # final reshape for transformation\n",
    "    grid = tf.reshape(grid, tf.stack([num_batch, 4, -1]))\n",
    "    \n",
    "    T_g = tf.matmul(thetas, grid)\n",
    "    input_transformed = _interpolate(U, T_g, out_size, min_ref_grid, max_ref_grid)\n",
    "    output = tf.reshape(input_transformed, tf.stack([num_batch, *out_size, num_channels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(321)\n",
    "batch = 0\n",
    "plt.imshow(output[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)\n",
    "plt.subplot(322)\n",
    "plt.imshow(U[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)\n",
    "\n",
    "plt.subplot(323)\n",
    "batch = 1\n",
    "plt.imshow(output[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)\n",
    "plt.subplot(324)\n",
    "plt.imshow(U[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)\n",
    "\n",
    "plt.subplot(325)\n",
    "batch = 2\n",
    "plt.imshow(output[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)\n",
    "plt.subplot(326)\n",
    "plt.imshow(U[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[1,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(U[0,:,:,0,0]-output[0,:,:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolation on a 3D grid \n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "x_ref_min = np.array([0, 0., 0.], dtype=np.float32)\n",
    "x_ref_max = np.array([np.pi, np.pi, np.pi], dtype=np.float32)\n",
    "ny = [100, 100, 100]\n",
    "\n",
    "# Build y_ref.\n",
    "x0s, x1s, x2s = tf.meshgrid(\n",
    "    tf.linspace(x_ref_min[0], x_ref_max[0], ny[0]),\n",
    "    tf.linspace(x_ref_min[1], x_ref_max[1], ny[1]),\n",
    "    tf.linspace(x_ref_min[2], x_ref_max[2], ny[2]),\n",
    "    indexing='ij')\n",
    "\n",
    "def func(x0, x1, x2):\n",
    "  # Shape [..., 2] output.\n",
    "  return tf.sin(x0) + tf.cos(x1) + tf.cos(x2)\n",
    "\n",
    "# Shape ny + [2]\n",
    "y_ref = func(x0s, x1s, x2s)\n",
    "\n",
    "# Shape [10, 2]\n",
    "seed = 0\n",
    "x = tf.stack([tf.random.uniform(shape=(10,), minval=x_ref_min[0], maxval=x_ref_max[0], seed=seed),\n",
    "              tf.random.uniform(shape=(10,), minval=x_ref_min[1], maxval=x_ref_max[1], seed=seed),\n",
    "              tf.random.uniform(shape=(10,), minval=x_ref_min[2], maxval=x_ref_max[2], seed=seed),\n",
    "            ], axis=-1)\n",
    "\n",
    "print(x.shape)\n",
    "print(y_ref.shape)\n",
    "expected_y = func(x[:, 0], x[:, 1], x[:, 2])\n",
    "actual_y = tfp.math.batch_interp_regular_nd_grid(\n",
    "    x=x, x_ref_min=x_ref_min, x_ref_max=x_ref_max, y_ref=y_ref, axis=-3)\n",
    "print(expected_y)\n",
    "print(actual_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src = tf.keras.Input(shape=(9, 9, 9, 1))\n",
    "inp_flatten = tf.keras.layers.Flatten()(src)\n",
    "conv = tf.keras.layers.Dense(units=7, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=0), activation=None)(inp_flatten)\n",
    "reg_out = LinearTransformation()([src, conv])\n",
    "model = tf.keras.Model(inputs=[src], outputs=[reg_out])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3)\n",
    "              , loss=[\"mae\"]\n",
    "              , metrics=[\"mae\"])\n",
    "\n",
    "res = model.predict(x=[x, trf], batch_size=1)\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "\n",
    "src = tf.keras.Input(shape=(220, 220, 220))\n",
    "y = tf.keras.Input(shape=(7))\n",
    "reg_out = LinearTransformation()([src, y])\n",
    "model = tf.keras.Model(inputs=[src, y], outputs=[reg_out])\n",
    "res = model.predict(x=[x, trf], batch_size=2)\n",
    "print(len(res))\n",
    "\n",
    "ElpsTime = time.time() - tic\n",
    "print(\"*** Total %1.3f s ***\"%(ElpsTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving output\n",
    "!rm data/vol*\n",
    "\n",
    "def save_array_to_sitk(data, name, data_dir):\n",
    "    ref_grid = create_ref_grid()\n",
    "    sitk_img = utils.get_sitk_from_numpy(data, ref_grid)\n",
    "    sitk.WriteImage(sitk_img, os.path.join(data_dir, name + \".nii.gz\"))\n",
    "\n",
    "for vol in range(res.shape[0]):\n",
    "    save_array_to_sitk(data=res[vol,], name=\"vol%02d\" %(vol+1), data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quaternion rotate here\n",
      "*** Total current nn 5.587 s ***\n",
      "*** Total previous 36.138 s ***\n",
      "*** Total current bilinear 32.896 s ***\n",
      "*** Total 32.899 s ***\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append('/home/ltetrel/DeepNeuroAN/deepneuroan/')\n",
    "sys.path.append('/home/ltetrel/DeepNeuroAN/')\n",
    "sys.path.append('/home/ltetrel/Documents/work/DeepNeuroAN/deepneuroan/')\n",
    "sys.path.append('/home/ltetrel/Documents/work/DeepNeuroAN/')\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "import tensorflow as tf\n",
    "\n",
    "from preproc import create_ref_grid\n",
    "import deepneuroan.utils as utils\n",
    "import SimpleITK as sitk\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def mem_size(var):\n",
    "    return (tf.size(var).numpy() * sys.getsizeof(var.dtype))/(1024**3)\n",
    "\n",
    "# inspired from https://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/layers/spatial_transformer.html\n",
    "def _repeat(x, n_repeats):\n",
    "    rep = tf.transpose(a=tf.expand_dims(tf.ones(\n",
    "                                                shape=tf.stack([n_repeats,])\n",
    "                                                ), 1)\n",
    "                       , perm=[1, 0])\n",
    "    rep = tf.cast(rep, dtype=tf.int32)\n",
    "    x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n",
    "    return tf.reshape(x, [-1])\n",
    "\n",
    "def _prev_interpolate(im, p, out_size, min_ref_grid, max_ref_grid):\n",
    "\n",
    "    # getting position coords as arrays\n",
    "    x = tf.reshape(tf.slice(p, [0, 0, 0], [-1, 1, -1]), [-1])\n",
    "    y = tf.reshape(tf.slice(p, [0, 1, 0], [-1, 1, -1]), [-1])\n",
    "    z = tf.reshape(tf.slice(p, [0, 2, 0], [-1, 1, -1]), [-1])\n",
    "\n",
    "    # constants\n",
    "    num_batch = tf.shape(im)[0]\n",
    "    height = tf.shape(im)[1]\n",
    "    width = tf.shape(im)[2]\n",
    "    depth = tf.shape(im)[3]\n",
    "    channels = tf.shape(im)[4]\n",
    "    out_height = out_size[0]\n",
    "    out_width = out_size[1]\n",
    "    out_depth = out_size[2]\n",
    "    height_f = tf.cast(height, dtype=tf.float32)\n",
    "    width_f = tf.cast(width, dtype=tf.float32)\n",
    "    depth_f = tf.cast(depth, dtype=tf.float32)\n",
    "    zero = tf.zeros([], dtype=tf.float32)\n",
    "\n",
    "    # scale positions to [0, width/height - 1]\n",
    "    ix = (x - min_ref_grid[0]) * (width_f - 1.)/(max_ref_grid[0] - min_ref_grid[0])\n",
    "    iy = (y - min_ref_grid[1]) * (height_f - 1.)/(max_ref_grid[1] - min_ref_grid[1])\n",
    "    iz = (z - min_ref_grid[2]) * (depth_f - 1.)/(max_ref_grid[2] - min_ref_grid[2])\n",
    "\n",
    "    # border padding mode, for positions outside of refrence grid\n",
    "    ix = tf.clip_by_value(ix, zero, width_f - 1)\n",
    "    iy = tf.clip_by_value(iy, zero, height_f - 1)\n",
    "    iz = tf.clip_by_value(iz, zero, depth_f - 1)\n",
    "\n",
    "    # get corner indexes based on the scaled positions\n",
    "    # nwt stands for north-west-top; seb stands for south-east-bottom\n",
    "    ix_nwt = tf.floor(ix)\n",
    "    iy_nwt = tf.floor(iy)\n",
    "    iz_nwt = tf.floor(iz)\n",
    "    ix_net = ix_nwt + 1\n",
    "    iy_net = iy_nwt\n",
    "    iz_net = iz_nwt\n",
    "    ix_swt = ix_nwt\n",
    "    iy_swt = iy_nwt + 1\n",
    "    iz_swt = iz_nwt\n",
    "    ix_set = ix_nwt + 1\n",
    "    iy_set = iy_nwt + 1\n",
    "    iz_set = iz_nwt\n",
    "    ix_nwb = ix_nwt\n",
    "    iy_nwb = iy_nwt\n",
    "    iz_nwb = iz_nwt + 1\n",
    "    ix_neb = ix_nwt + 1\n",
    "    iy_neb = iy_nwt\n",
    "    iz_neb = iz_nwt + 1\n",
    "    ix_swb = ix_nwt\n",
    "    iy_swb = iy_nwt + 1\n",
    "    iz_swb = iz_nwt + 1\n",
    "    ix_seb = ix_nwt + 1\n",
    "    iy_seb = iy_nwt + 1\n",
    "    iz_seb = iz_nwt + 1\n",
    "\n",
    "    # calculate the weights for each p position\n",
    "    nwt = tf.expand_dims(((ix_seb - ix)    * (iy_seb - iy)    * (iz_seb - iz)), 1)\n",
    "    net = tf.expand_dims(((ix    - ix_swb) * (iy_swb - iy)    * (iz_swb - iz)), 1)\n",
    "    swt = tf.expand_dims(((ix_neb - ix)    * (iy    - iy_neb) * (iz_neb - iz)), 1)\n",
    "    set = tf.expand_dims(((ix    - ix_nwb) * (iy    - iy_nwb) * (iz_nwb - iz)), 1)\n",
    "    nwb = tf.expand_dims(((ix_set - ix)    * (iy_set - iy)    * (iz - iz_set)), 1)\n",
    "    neb = tf.expand_dims(((ix    - ix_swt) * (iy_swt - iy)    * (iz - iz_swt)), 1)\n",
    "    swb = tf.expand_dims(((ix_net - ix)    * (iy    - iy_net) * (iz - iz_net)), 1)\n",
    "    seb = tf.expand_dims(((ix    - ix_nwt) * (iy    - iy_nwt) * (iz - iz_nwt)), 1)\n",
    "\n",
    "    # gather input img values from positions\n",
    "    # get corners idx\n",
    "    ibatch = _repeat(tf.range(num_batch) * height * width * depth, out_height * out_width * out_depth)\n",
    "    idx_nwt = ibatch + tf.cast(tf.clip_by_value(ix_nwt, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_nwt, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_nwt, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_net = ibatch + tf.cast(tf.clip_by_value(ix_net, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_net, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_net, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_swt = ibatch + tf.cast(tf.clip_by_value(ix_swt, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_swt, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_swt, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_set = ibatch + tf.cast(tf.clip_by_value(ix_set, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_set, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_set, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_nwb = ibatch + tf.cast(tf.clip_by_value(ix_nwb, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_nwb, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_nwb, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_neb = ibatch + tf.cast(tf.clip_by_value(ix_neb, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_neb, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_neb, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_swb = ibatch + tf.cast(tf.clip_by_value(ix_swb, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_swb, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_swb, zero, depth_f - 1), tf.int32) * width * height\n",
    "    idx_seb = ibatch + tf.cast(tf.clip_by_value(ix_seb, zero, width_f - 1), tf.int32) + tf.cast(tf.clip_by_value(iy_seb, zero, height_f - 1), tf.int32) * width + tf.cast(tf.clip_by_value(iz_seb, zero, depth_f - 1), tf.int32) * width * height\n",
    " \n",
    "    # gather input image values from idx\n",
    "    im_flat = tf.reshape(im, tf.stack([-1, channels]))\n",
    "    I_nwt = tf.gather(im_flat, idx_nwt)\n",
    "    I_net = tf.gather(im_flat, idx_net)\n",
    "    I_swt = tf.gather(im_flat, idx_swt)\n",
    "    I_set = tf.gather(im_flat, idx_set)\n",
    "    I_nwb = tf.gather(im_flat, idx_nwb)\n",
    "    I_neb = tf.gather(im_flat, idx_neb)\n",
    "    I_swb = tf.gather(im_flat, idx_swb)\n",
    "    I_seb = tf.gather(im_flat, idx_seb)\n",
    "    \n",
    "    output = tf.add_n([nwt * I_nwt, net * I_net, swt * I_swt, set * I_set, nwb * I_nwb, neb * I_neb, swb * I_swb, seb * I_seb])\n",
    "    return output\n",
    "\n",
    "def _interpolate(im, p, out_size, min_ref_grid, max_ref_grid, method=\"nn\"):\n",
    "\n",
    "    # constants\n",
    "    num_batch = tf.shape(im)[0]\n",
    "    height = tf.shape(im)[1]\n",
    "    width = tf.shape(im)[2]\n",
    "    depth = tf.shape(im)[3]\n",
    "    channels = tf.shape(im)[4]\n",
    "    out_height = out_size[0]\n",
    "    out_width = out_size[1]\n",
    "    out_depth = out_size[2]\n",
    "    height_f = tf.cast(height, dtype=tf.float32)\n",
    "    width_f = tf.cast(width, dtype=tf.float32)\n",
    "    depth_f = tf.cast(depth, dtype=tf.float32)\n",
    "    zero = tf.zeros([], dtype=tf.float32)\n",
    "    im_flat = tf.reshape(im, tf.stack([-1, channels]))\n",
    "\n",
    "    # scale positions to [0, width/height - 1]\n",
    "    ix = (tf.reshape(p[:, 0, :], [-1]) - min_ref_grid[0]) * (width_f - 1.)/(max_ref_grid[0] - min_ref_grid[0])\n",
    "    iy = (tf.reshape(p[:, 1, :], [-1]) - min_ref_grid[1]) * (height_f - 1.)/(max_ref_grid[1] - min_ref_grid[1])\n",
    "    iz = (tf.reshape(p[:, 2, :], [-1]) - min_ref_grid[2]) * (depth_f - 1.)/(max_ref_grid[2] - min_ref_grid[2])\n",
    "\n",
    "    # border padding mode, for positions outside of refrence grid\n",
    "    ix = tf.clip_by_value(ix, zero, width_f - 1)\n",
    "    iy = tf.clip_by_value(iy, zero, height_f - 1)\n",
    "    iz = tf.clip_by_value(iz, zero, depth_f - 1)\n",
    "\n",
    "    # if we use bilinear interpolation, we calculate each area between corners and positions to get the weights for each input pixel\n",
    "    if method == \"bilinear\":\n",
    "        # get corner indexes based on the scaled positions\n",
    "        # nwt corner stands for north-west-top; seb stands for south-east-bottom\n",
    "        ix_nwt = tf.floor(ix)\n",
    "        iy_nwt = tf.floor(iy)\n",
    "        iz_nwt = tf.floor(iz)\n",
    "        ix_net = ix_nwt + 1.\n",
    "        iy_net = iy_nwt\n",
    "        iz_net = iz_nwt\n",
    "        ix_swt = ix_nwt\n",
    "        iy_swt = iy_nwt + 1.\n",
    "        iz_swt = iz_nwt\n",
    "        ix_set = ix_nwt + 1.\n",
    "        iy_set = iy_nwt + 1.\n",
    "        iz_set = iz_nwt\n",
    "        ix_nwb = ix_nwt\n",
    "        iy_nwb = iy_nwt\n",
    "        iz_nwb = iz_nwt + 1.\n",
    "        ix_neb = ix_nwt + 1.\n",
    "        iy_neb = iy_nwt\n",
    "        iz_neb = iz_nwt + 1.\n",
    "        ix_swb = ix_nwt\n",
    "        iy_swb = iy_nwt + 1.\n",
    "        iz_swb = iz_nwt + 1.\n",
    "        ix_seb = ix_nwt + 1.\n",
    "        iy_seb = iy_nwt + 1.\n",
    "        iz_seb = iz_nwt + 1.\n",
    "\n",
    "        # calculate the weights for each p position\n",
    "        nwt = tf.expand_dims(((ix_seb - ix)    * (iy_seb - iy)    * (iz_seb - iz)), 1)\n",
    "        net = tf.expand_dims(((ix    - ix_swb) * (iy_swb - iy)    * (iz_swb - iz)), 1)\n",
    "        swt = tf.expand_dims(((ix_neb - ix)    * (iy    - iy_neb) * (iz_neb - iz)), 1)\n",
    "        set = tf.expand_dims(((ix    - ix_nwb) * (iy    - iy_nwb) * (iz_nwb - iz)), 1)\n",
    "        nwb = tf.expand_dims(((ix_set - ix)    * (iy_set - iy)    * (iz - iz_set)), 1)\n",
    "        neb = tf.expand_dims(((ix    - ix_swt) * (iy_swt - iy)    * (iz - iz_swt)), 1)\n",
    "        swb = tf.expand_dims(((ix_net - ix)    * (iy    - iy_net) * (iz - iz_net)), 1)\n",
    "        seb = tf.expand_dims(((ix    - ix_nwt) * (iy    - iy_nwt) * (iz - iz_nwt)), 1)\n",
    "\n",
    "        # gather input pixel values from corners idx\n",
    "        ibatch = _repeat(tf.range(num_batch) * height * width * depth, out_height * out_width * out_depth)\n",
    "        idx_nwt = ibatch + tf.cast(tf.math.minimum(width_f - 1, ix_nwt), tf.int32) + tf.cast(tf.math.minimum(height_f - 1, iy_nwt), tf.int32) * width + tf.cast(tf.math.minimum(depth_f - 1, iz_nwt), tf.int32) * width * height\n",
    "        idx_net = ibatch + tf.cast(tf.math.minimum(width_f - 1, ix_net), tf.int32) + tf.cast(tf.math.minimum(height_f - 1, iy_net), tf.int32) * width + tf.cast(tf.math.minimum(depth_f - 1, iz_net), tf.int32) * width * height\n",
    "        idx_swt = ibatch + tf.cast(tf.math.minimum(width_f - 1, ix_swt), tf.int32) + tf.cast(tf.math.minimum(height_f - 1, iy_swt), tf.int32) * width + tf.cast(tf.math.minimum(depth_f - 1, iz_swt), tf.int32) * width * height\n",
    "        idx_set = ibatch + tf.cast(tf.math.minimum(width_f - 1, ix_set), tf.int32) + tf.cast(tf.math.minimum(height_f - 1, iy_set), tf.int32) * width + tf.cast(tf.math.minimum(depth_f - 1, iz_set), tf.int32) * width * height\n",
    "        idx_nwb = ibatch + tf.cast(tf.math.minimum(width_f - 1, ix_nwb), tf.int32) + tf.cast(tf.math.minimum(height_f - 1, iy_nwb), tf.int32) * width + tf.cast(tf.math.minimum(depth_f - 1, iz_nwb), tf.int32) * width * height\n",
    "        idx_neb = ibatch + tf.cast(tf.math.minimum(width_f - 1, ix_neb), tf.int32) + tf.cast(tf.math.minimum(height_f - 1, iy_neb), tf.int32) * width + tf.cast(tf.math.minimum(depth_f - 1, iz_neb), tf.int32) * width * height\n",
    "        idx_swb = ibatch + tf.cast(tf.math.minimum(width_f - 1, ix_swb), tf.int32) + tf.cast(tf.math.minimum(height_f - 1, iy_swb), tf.int32) * width + tf.cast(tf.math.minimum(depth_f - 1, iz_swb), tf.int32) * width * height\n",
    "        idx_seb = ibatch + tf.cast(tf.math.minimum(width_f - 1, ix_seb), tf.int32) + tf.cast(tf.math.minimum(height_f - 1, iy_seb), tf.int32) * width + tf.cast(tf.math.minimum(depth_f - 1, iz_seb), tf.int32) * width * height\n",
    "        \n",
    "        \n",
    "        # gather input image values from idx\n",
    "        I_nwt = tf.gather(im_flat, idx_nwt)\n",
    "        I_net = tf.gather(im_flat, idx_net)\n",
    "        I_swt = tf.gather(im_flat, idx_swt)\n",
    "        I_set = tf.gather(im_flat, idx_set)\n",
    "        I_nwb = tf.gather(im_flat, idx_nwb)\n",
    "        I_neb = tf.gather(im_flat, idx_neb)\n",
    "        I_swb = tf.gather(im_flat, idx_swb)\n",
    "        I_seb = tf.gather(im_flat, idx_seb)\n",
    "        \n",
    "        output = tf.add_n([nwt * I_nwt, net * I_net, swt * I_swt, set * I_set, nwb * I_nwb, neb * I_neb, swb * I_swb, seb * I_seb])\n",
    "    \n",
    "    # else if method is nearest neighbor, we get the pixel values from the north-west-top corner\n",
    "    elif method == \"nn\":\n",
    "        # get corner indexes based on the scaled positions\n",
    "        # nwt corner stands for north-west-top; seb stands for south-east-bottom\n",
    "        ix_nwt = tf.floor(ix)\n",
    "        iy_nwt = tf.floor(iy)\n",
    "        iz_nwt = tf.floor(iz)\n",
    "\n",
    "        # gather input pixel values from nwt corner idx\n",
    "        ibatch = _repeat(tf.range(num_batch) * height * width * depth, out_height * out_width * out_depth)\n",
    "        idx_nwt = ibatch + tf.cast(ix_nwt, tf.int32) + tf.cast(iy_nwt, tf.int32) * width + tf.cast(iz_nwt, tf.int32) * width * height\n",
    "        output = tf.gather(im_flat, idx_nwt)\n",
    "\n",
    "    return output\n",
    "\n",
    "n_batch = 3\n",
    "transfos = tf.random.uniform(shape=(n_batch, 7), seed=0, dtype=tf.float32) #quaternions (4,) + translations (3,) + scales (3,)\n",
    "transfos = tf.stack( [0.*tf.ones(shape=(7), dtype=tf.float32), (-10)*tf.ones(shape=(7), dtype=tf.float32), (-30)*tf.ones(shape=(7), dtype=tf.float32)] )\n",
    "U_single = 34*tf.random.uniform((1, 220, 220, 220, 1), seed=0, dtype=tf.float32)\n",
    "\n",
    "np.random.seed(0)\n",
    "U_single = 34*np.random.rand(1, 220, 220, 220, 1)\n",
    "U_single[:, 20:25, :, :, :] = 255.\n",
    "U_single[:, :, 20:25, :, :] = 255.\n",
    "U_single[:, :, :, 20:25, :] = 255.\n",
    "U_single[:, 195:200, :, :, :] = 255.\n",
    "U_single[:, :, 195:200, :, :] = 255.\n",
    "U_single[:, :, :, 195:200, :] = 255.\n",
    "U_single[:, :20, :, :, :] = 0.\n",
    "U_single[:, :, :20, :, :] = 0.\n",
    "U_single[:, :20, :, :, :] = 0.\n",
    "U_single[:, 200:, :, :, :] = 0.\n",
    "U_single[:, :, 200:, :, :] = 0.\n",
    "U_single[:, :, :, 200:, :] = 0.\n",
    "\n",
    "U = tf.tile( tf.cast(U_single, dtype=tf.float32), (n_batch, 1, 1, 1, 1), name=None)\n",
    "out_size = [220, 220, 220]\n",
    "\n",
    "\n",
    "# U_single = 34*tf.random.uniform((1, 15, 15, 15, 1), seed=0, dtype=tf.float32)\n",
    "\n",
    "# np.random.seed(seed=0)\n",
    "# U_single = 34*np.random.rand(1, 15, 15, 15, 1)\n",
    "# U_single[:, 1:2, :, :, :] = 255.\n",
    "# U_single[:, :, 1:2, :, :] = 255.\n",
    "# U_single[:, :, :, 1:2, :] = 255.\n",
    "# U_single[:, 13:14, :, :, :] = 255.\n",
    "# U_single[:, :, 13:14, :, :] = 255.\n",
    "# U_single[:, :, :, 13:14, :] = 255.\n",
    "# U_single[:, :1, :, :, :] = 0.\n",
    "# U_single[:, :, :1, :, :] = 0.\n",
    "# # U_single[:, :, :, :4, :] = 0.\n",
    "# U_single[:, 14:, :, :, :] = 0.\n",
    "# U_single[:, :, 14:, :, :] = 0.\n",
    "# # U_single[:, :, :, 36:, :] = 0.\n",
    "\n",
    "# U = tf.tile( tf.cast(U_single, dtype=tf.float32), (n_batch, 1, 1, 1, 1), name=None)\n",
    "# out_size = [15, 15, 15]\n",
    "\n",
    "name='BatchSpatialTransformer3dAffine'\n",
    "\n",
    "ref_size = tf.shape(U)[1:-1]\n",
    "# TODO: min[d] and max[d] correspond to cartesian coordinate d (d=0 is x, d=1 is y ..)\n",
    "min_ref_grid = tf.constant([-30., -10., 15.])\n",
    "max_ref_grid = tf.constant([-18., 10., 37])\n",
    "# min_ref_grid = tf.constant([0., 0., 0.], dtype=tf.float32)\n",
    "# max_ref_grid = tf.constant(tf.stack(tf.cast(ref_size, dtype=tf.float32) - 1), dtype=tf.float32)\n",
    "\n",
    "import time\n",
    "tic = time.time()\n",
    "\n",
    "with tf.compat.v1.variable_scope(name):\n",
    "    input_dim = U\n",
    "    \n",
    "    num_batch = tf.shape(input=input_dim)[0]\n",
    "    num_channels = tf.shape(input=input_dim)[-1]\n",
    "    \n",
    "    #if the transformations has length > 7, then we apply scaling\n",
    "    if tf.shape(transfos)[-1] > 7:\n",
    "        thetas = tf.linalg.diag(transfos[:, -3:])\n",
    "    else:\n",
    "        thetas = tf.eye(num_rows=3, batch_shape=[num_batch])\n",
    "        \n",
    "    #if the transformations has length > 4, then we apply translation\n",
    "    if tf.shape(transfos)[-1] > 4:  \n",
    "        thetas = tf.concat(axis=2, values=[thetas, transfos[:, 4:7, tf.newaxis]])\n",
    "    else:\n",
    "        thetas = tf.concat(axis=2, values=[thetas, tf.zeros((num_batch, 3, 1))])\n",
    "    \n",
    "    # physical points of source volume, from template affine\n",
    "    # if we don't have volume affine, we use a grid [-1, 1]\n",
    "    if (min_ref_grid is None) | (max_ref_grid is None):\n",
    "        min_ref_grid = (-1)*tf.ones(3)\n",
    "        max_ref_grid = tf.ones(3)\n",
    "    m_y, m_z, m_x = tf.meshgrid(tf.linspace(min_ref_grid[1], max_ref_grid[1], ref_size[0]),\n",
    "                                tf.linspace(min_ref_grid[2], max_ref_grid[2], ref_size[2]),\n",
    "                                tf.linspace(min_ref_grid[0], max_ref_grid[0], ref_size[1]),\n",
    "                                indexing='xy')\n",
    "    \n",
    "    # physical points for quaternion\n",
    "    grid = tf.concat(axis=0, values=[tf.reshape(m_x, (1, -1)), tf.reshape(m_y, (1, -1)), tf.reshape(m_z, (1, -1))])\n",
    "    grid = tf.transpose(grid)\n",
    "    print(\"quaternion rotate here\")\n",
    "#     grid = tfg.geometry.transformation.quaternion.rotate(grid, transfos[:, :4])\n",
    "\n",
    "    grid = tf.transpose(grid)\n",
    "    grid = tf.concat(axis=0, values=[grid, tf.ones_like(tf.reshape(m_x, (1, -1)))])\n",
    "    # adding batch dimension\n",
    "    grid = tf.expand_dims(grid, 0)\n",
    "    grid = tf.reshape(grid, [-1])\n",
    "    grid = tf.tile(grid, tf.stack([num_batch]))\n",
    "    # final reshape for transformation\n",
    "    grid = tf.reshape(grid, tf.stack([num_batch, 4, -1]))\n",
    "    \n",
    "    T_g = tf.matmul(thetas, grid)\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in range(10):\n",
    "        input_transformed = _interpolate(U, T_g, out_size, min_ref_grid, max_ref_grid)\n",
    "    ElpsTime = time.time() - tic\n",
    "    print(\"*** Total current nn %1.3f s ***\"%(ElpsTime))\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in range(10):\n",
    "        input_transformed = _prev_interpolate(U, T_g, out_size, min_ref_grid, max_ref_grid)\n",
    "    ElpsTime = time.time() - tic\n",
    "    print(\"*** Total previous %1.3f s ***\"%(ElpsTime))\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in range(10):\n",
    "        input_transformed = _interpolate(U, T_g, out_size, min_ref_grid, max_ref_grid, method=\"bilinear\")\n",
    "    ElpsTime = time.time() - tic\n",
    "    print(\"*** Total current bilinear %1.3f s ***\"%(ElpsTime))\n",
    "\n",
    "    output = tf.reshape(input_transformed, tf.stack([num_batch, *out_size, num_channels]))\n",
    "\n",
    "ElpsTime = time.time() - tic\n",
    "print(\"*** Total %1.3f s ***\"%(ElpsTime))\n",
    "\n",
    "# from vprof import runner\n",
    "# runner.run(my_func, 'cmhp', host='localhost', port=8000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(321)\n",
    "batch = 0\n",
    "plt.imshow(output[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)\n",
    "plt.subplot(322)\n",
    "plt.imshow(U[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)\n",
    "\n",
    "plt.subplot(323)\n",
    "batch = 1\n",
    "plt.imshow(output[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)\n",
    "plt.subplot(324)\n",
    "plt.imshow(U[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)\n",
    "\n",
    "plt.subplot(325)\n",
    "batch = 2\n",
    "plt.imshow(output[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)\n",
    "plt.subplot(326)\n",
    "plt.imshow(U[batch,:,:,0,0], cmap=\"gray\", vmin=0, vmax=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Total 0.987 s ***\n"
     ]
    }
   ],
   "source": [
    "# trying with a simple transform\n",
    "t_np = np.array([[1, 0, 0, 0]\n",
    "                 ,[0, 1, 0, -10]\n",
    "                 ,[0, 0, 1, -30]\n",
    "                 ,[0, 0, 0, 1]])\n",
    "\n",
    "affine = sitk.Euler3DTransform()\n",
    "affine.SetTranslation((-10, 30, 0))\n",
    "\n",
    "data_dir = \"./data\"\n",
    "filepath = data_dir + \"/ses-vid001_task-video_run-01_bold_vol-0001_transfo-%06d.nii.gz\" %(1)\n",
    "ref_grid = create_ref_grid()\n",
    "source_brain = sitk.ReadImage(filepath, sitk.sitkFloat32)\n",
    "\n",
    "import time\n",
    "tic = time.time()\n",
    "for i in range(30):\n",
    "    brain_to_grid = sitk.Resample(source_brain, ref_grid, affine, sitk.sitkLinear, 0.0, sitk.sitkFloat32)\n",
    "ElpsTime = time.time() - tic\n",
    "print(\"*** Total %1.3f s ***\"%(ElpsTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
