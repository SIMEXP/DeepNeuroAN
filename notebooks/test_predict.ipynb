{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import nibabel as nib\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append(\"../\")\n",
    "from deepneuroan.data_generator import DataGenerator\n",
    "from deepneuroan.models import ChannelwiseConv3D\n",
    "from deepneuroan.models import rigid_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/generated_data/\"\n",
    "template_filepath = os.path.join(data_dir, \"template_on_grid\")\n",
    "\n",
    "list_files = []\n",
    "list_files_tmp = set([])\n",
    "for root, _, files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        filepath = os.path.join(root, file).split('.')[0]\n",
    "        if os.path.exists(filepath + \".txt\"):\n",
    "            list_files_tmp.add(filepath)\n",
    "list_files = list(list_files_tmp)\n",
    "\n",
    "bs = 1\n",
    "ncpu = 1\n",
    "np.random.seed(0)\n",
    "params_gen = dict(list_files=list_files, template_file=template_filepath, batch_size=bs, avail_cores=ncpu)\n",
    "all_gen = DataGenerator(partition=\"all\", **params_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/rigid_concatenated.json\", \"r\") as json_file:\n",
    "    model = tf.keras.models.model_from_json(json_file.read(), custom_objects={'ChannelwiseConv3D': ChannelwiseConv3D})\n",
    "model.load_weights(\"/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/model_bs16_lr0.01_2019-12-12_10:10:04.h5\", by_name=False)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01)\n",
    "                  , loss=tf.keras.losses.mean_squared_error\n",
    "                  , metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/generated_data/ses-vid001_task-video_run-01_bold_vol-202_transfo-000002']\n",
      "current :  [[ 0.021478  0.236571  0.243625 -0.94033  -0.725687 -1.267853  1.380169]]\n",
      "ae :  [[ 0.38195992 -0.258983    0.25626796  1.2763      0.846229    0.85686064\n",
      "   0.22063828]]\n",
      "mae :  0.5853198\n",
      "['/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/generated_data/ses-vid001_task-video_run-01_bold_vol-202_transfo-000007']\n",
      "current :  [[ 0.940106 -0.191497 -0.649573 -0.078666  1.664281 -0.407973 -0.870109]]\n",
      "ae :  [[-0.23970908  0.1838034   0.2204777   0.06258649 -1.9983807   0.7531328\n",
      "   0.9947016 ]]\n",
      "mae :  0.6361131\n",
      "['/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/generated_data/ses-vid001_task-video_run-01_bold_vol-202_transfo-000005']\n",
      "current :  [[ 0.582358 -0.749234  0.453074 -1.369968  0.367358  0.356667  3.865819]]\n",
      "ae :  [[-0.05909014  0.48220924 -0.49004593  0.4577881  -0.58346736  0.23217788\n",
      "   0.8258772 ]]\n",
      "mae :  0.44723657\n",
      "['/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/generated_data/ses-vid001_task-video_run-01_bold_vol-202_transfo-000008']\n",
      "current :  [[ 0.626122 -0.608823 -1.276217 -0.65764   0.487344 -1.478345  0.066601]]\n",
      "ae :  [[ 0.1265167   0.32987577  1.2359488   0.91144836 -0.09774789  0.59632164\n",
      "   0.27706313]]\n",
      "mae :  0.5107032\n",
      "['/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/generated_data/ses-vid001_task-video_run-01_bold_vol-202_transfo-000004']\n",
      "current :  [[ 0.559889 -1.649922  0.150021  0.036711  2.047311  0.723978 -1.093582]]\n",
      "ae :  [[ 0.0961619   0.5737835   0.23649047  0.06411804 -1.9409989  -0.843675\n",
      "   0.0164727 ]]\n",
      "mae :  0.5388144\n",
      "['/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/generated_data/ses-vid001_task-video_run-01_bold_vol-202_transfo-000001']\n",
      "current :  [[ 0.736927 -0.51255   0.926026 -0.841142  1.158329 -0.217082 -2.097407]]\n",
      "ae :  [[-0.17860883  0.17891344 -0.4356661   0.10113966 -0.51449794  0.7901695\n",
      "   0.30090535]]\n",
      "mae :  0.35712868\n",
      "['/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/generated_data/ses-vid001_task-video_run-01_bold_vol-202_transfo-000010']\n",
      "current :  [[ 0.973435 -0.430999 -0.154067 -0.014246  3.909032  2.703077 -0.151436]]\n",
      "ae :  [[-0.30185395  0.2901633  -0.13988934 -0.39769703 -3.0545278   0.19020915\n",
      "   0.27035594]]\n",
      "mae :  0.66352814\n",
      "['/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/generated_data/ses-vid001_task-video_run-01_bold_vol-202_transfo-000003']\n",
      "current :  [[ 0.836988 -0.162586 -0.561283  0.925383  0.354705 -1.573839 -0.306913]]\n",
      "ae :  [[-0.11155218  0.22021392  0.21311465  0.07850409 -1.1640769   0.12524772\n",
      "   0.714821  ]]\n",
      "mae :  0.3753615\n",
      "['/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/generated_data/ses-vid001_task-video_run-01_bold_vol-202_transfo-000009']\n",
      "current :  [[  0.857114  -0.42964   -0.925611   0.141676  24.199486 -10.192649\n",
      "  -18.795437]]\n",
      "ae :  [[-0.05293691  0.27197123  0.7033514  -0.718354   -8.653223   -4.029291\n",
      "  -1.6224098 ]]\n",
      "mae :  2.2930768\n",
      "['/home/ltetrel/Documents/data/neuromod/derivatives/deepneuroan/training/generated_data/ses-vid001_task-video_run-01_bold_vol-202_transfo-000006']\n",
      "current :  [[ 0.574592 -1.060401 -0.279431  1.215256  0.314142  0.99452   0.624101]]\n",
      "ae :  [[ 0.0045104   0.27802163  0.47681296  0.00662136 -0.3192844  -0.30308557\n",
      "  -0.39310274]]\n",
      "mae :  0.2544913\n"
     ]
    }
   ],
   "source": [
    "for i in range(all_gen.__len__()):\n",
    "    print(all_gen.get_files_batch(i))\n",
    "    error = model.predict(x=all_gen.__getitem__(i)[0], use_multiprocessing=False, verbose=0) - all_gen.__getitem__(i)[1]\n",
    "    print(\"current : \", all_gen.__getitem__(i)[1])\n",
    "    print(\"ae : \", error)\n",
    "    print(\"mae : \", model.evaluate(x=all_gen.__getitem__(i)[0], y=all_gen.__getitem__(i)[1], batch_size=bs, use_multiprocessing=False, verbose=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.734801"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(all_gen.__getitem__(i)[1][0][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999997238275589"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm([ 0.0225887, 0.00184996, -0.00810505, 0.99971 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.574592, -1.060401, -0.279431,  1.215256], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_gen.__getitem__(i)[1][0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://itk-insight-users.2283740.n2.nabble.com/to-understand-quaternion-versor-and-matrix-in-VersorRigid3DTransform-td2510548.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
