{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import platform\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('/home/ltetrel/DeepNeuroAN/deepneuroan/')\n",
    "sys.path.append('/home/ltetrel/DeepNeuroAN/')\n",
    "\n",
    "from data_generator import DataGenerator\n",
    "import models\n",
    "import metrics\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1)\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self\n",
    "                 , data_dir=None\n",
    "                 , ckpt_dir=None\n",
    "                 , model_path=None\n",
    "                 , output_model_path=None\n",
    "                 , model_name=\"rigid_concatenated\"\n",
    "                 , weights_dir=None\n",
    "                 , seed=None\n",
    "                 , epochs=50\n",
    "                 , batch_size=8\n",
    "                 , kernel_size=[3, 3, 3]\n",
    "                 , pool_size=[2, 2, 2]\n",
    "                 , dilation=[1, 1, 1]\n",
    "                 , strides=[2, 2, 2]\n",
    "                 , activation=\"relu\"\n",
    "                 , padding=\"VALID\"\n",
    "                 , no_batch_norm=False\n",
    "                 , preproc_layers=0\n",
    "                 , motion_correction = False\n",
    "                 , unsupervised = False\n",
    "                 , dropout=0\n",
    "                 , growth_rate=2\n",
    "                 , filters=4\n",
    "                 , units=1024\n",
    "                 , encode_layers=7\n",
    "                 , regression_layers=4\n",
    "                 , lr=1e-4\n",
    "                 , gpu=-1\n",
    "                 , ncpu=-1):\n",
    "        self._model_path = model_path\n",
    "        self._model_name = model_name\n",
    "        self._weights_dir = weights_dir\n",
    "        self._epochs = epochs\n",
    "        self._kernel_size = tuple(kernel_size)\n",
    "        self._pool_size = tuple(pool_size)\n",
    "        self._dilation = tuple(dilation)\n",
    "        self._strides = tuple(strides)\n",
    "        self._batch_size = int(batch_size)\n",
    "        self._activation = activation\n",
    "        self._padding = padding\n",
    "        self._batch_norm = not no_batch_norm\n",
    "        self._preproc_layers = preproc_layers\n",
    "        self._use_template = not motion_correction\n",
    "        self._unsupervised = unsupervised\n",
    "        self._dropout = float(dropout)\n",
    "        self._growth_rate = float(growth_rate)\n",
    "        self._filters = int(filters)\n",
    "        self._units = int(units)\n",
    "        self._encode_layers = int(encode_layers)\n",
    "        self._regression_layers = int(regression_layers)\n",
    "        self._lr = lr\n",
    "        self._gpu = gpu\n",
    "        self._ncpu = ncpu\n",
    "\n",
    "        self._data_dir = None\n",
    "        self._ckpt_dir = None\n",
    "        self._ckpt_path = None\n",
    "        self._output_model_path = None\n",
    "        self._list_files = None\n",
    "        self._seed = None\n",
    "\n",
    "        self._set_data_dir(data_dir)\n",
    "        self._set_seed(seed)\n",
    "        self._set_list_files()\n",
    "        self._set_ckpt_dir(ckpt_dir)\n",
    "        self._set_output_model_path(output_model_path)\n",
    "        self._set_ncpu()\n",
    "\n",
    "        self.train_gen = None\n",
    "        self.valid_gen = None\n",
    "        self.test_gen = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(__file__) \\\n",
    "               + \"\\n\" + str(datetime.datetime.now()) \\\n",
    "               + \"\\n\" + str(platform.platform()) \\\n",
    "               + \"\\n\" + \"class Training()\" \\\n",
    "               + \"\\n\\t input data dir : %s\" % self._data_dir \\\n",
    "               + \"\\n\\t checkpoint dir : %s\" % self._ckpt_dir \\\n",
    "               + \"\\n\\t model name : %s\" % self._model_name \\\n",
    "               + \"\\n\\t weights dir : %s\" % self._weights_dir \\\n",
    "               + \"\\n\\t seed : %s\" % self._seed \\\n",
    "               + \"\\n\\t number of epochs : %s\" % (self._epochs,) \\\n",
    "               + \"\\n\\t batch size : %s\" % self._batch_size \\\n",
    "               + \"\\n\\t kernel size : %s\" % (self._kernel_size,) \\\n",
    "               + \"\\n\\t pool size : %s\" % (self._pool_size,) \\\n",
    "               + \"\\n\\t dilation rate : %s\" % (self._dilation,) \\\n",
    "               + \"\\n\\t strides : %s\" % (self._strides,) \\\n",
    "               + \"\\n\\t padding : %s\" % self._padding \\\n",
    "               + \"\\n\\t activation : %s\" % self._activation \\\n",
    "               + \"\\n\\t batch norm : %s\" % self._batch_norm \\\n",
    "               + \"\\n\\t preprocessing (gaussian) layers : %s\" % self._preproc_layers \\\n",
    "               + \"\\n\\t motion correction : %s\" % (not self._use_template) \\\n",
    "               + \"\\n\\t unsupervised learning : %s\" % (self._unsupervised) \\\n",
    "               + \"\\n\\t dropout : %f\" % self._dropout \\\n",
    "               + \"\\n\\t growth rate : %d\" % self._growth_rate \\\n",
    "               + \"\\n\\t filters : %d\" % self._filters \\\n",
    "               + \"\\n\\t units : %d\" % self._units \\\n",
    "               + \"\\n\\t number of encoding layer : %d\" % self._encode_layers \\\n",
    "               + \"\\n\\t number of regression layer : %d\" % self._regression_layers \\\n",
    "               + \"\\n\\t learning rate : %f\" % self._lr \\\n",
    "               + \"\\n\\t number of cpus : %d\" % self._ncpu \\\n",
    "               + \"\\n\\t gpu : %d\" % self._gpu\n",
    "\n",
    "    def _set_data_dir(self, data_dir=None):\n",
    "        if data_dir is None:\n",
    "            self._data_dir = os.getcwd()\n",
    "        else:\n",
    "            self._data_dir = data_dir\n",
    "\n",
    "    def _set_ckpt_dir(self, ckpt_dir=None):\n",
    "        if (ckpt_dir is None) & (self._data_dir is not None):\n",
    "            self._ckpt_dir = os.path.join(self._data_dir, \"../\", \"checkpoints\")\n",
    "        else:\n",
    "            self._ckpt_dir = ckpt_dir\n",
    "        self._ckpt_path = os.path.join(self._ckpt_dir, \"%s\" % self._model_name, \"%s_cp-{epoch:04d}.ckpt\" % self._model_name)\n",
    "\n",
    "    def _set_ncpu(self):\n",
    "        ncpu = self._ncpu\n",
    "        if ncpu < 0:\n",
    "            ncpu = os.cpu_count()\n",
    "        elif ncpu == 0:\n",
    "            ncpu = 1\n",
    "        self._ncpu = ncpu\n",
    "\n",
    "    def _set_output_model_path(self, output_model_path=None):\n",
    "        if (output_model_path is None) & (self._data_dir is not None):\n",
    "            self._output_model_path = os.path.join(\n",
    "                self._data_dir, \"../\", \"%s_{end_time:s}\" % self._model_name)\n",
    "        else:\n",
    "            self._output_model_path = output_model_path\n",
    "\n",
    "    def _set_seed(self, seed=None):\n",
    "        if seed is not None:\n",
    "            self._seed = int(seed)\n",
    "\n",
    "    def _set_list_files(self):\n",
    "        self._list_files = []\n",
    "        list_files_tmp = set([])\n",
    "        for root, _, files in os.walk(self._data_dir):\n",
    "            for file in files:\n",
    "                filepath = os.path.join(root, file).split('.')[0]\n",
    "                if os.path.exists(filepath + \".txt\"):\n",
    "                    list_files_tmp.add(filepath)\n",
    "        self._list_files = list(list_files_tmp)\n",
    "        self._list_files.sort()\n",
    "\n",
    "    def _build_model(self):\n",
    "        if self._model_path is not None:\n",
    "            if self._model_path.split(\".\")[-1] == \"json\":\n",
    "                with open(self._model_path, \"r\") as json_file:\n",
    "                    model = tf.keras.models.model_from_json(json_file.read(), custom_objects={'ChannelwiseConv3D': models.ChannelwiseConv3D})\n",
    "            elif self._model_path.split(\".\")[-1] == \"h5\":\n",
    "                model = tf.keras.models.load_model(self._model_path, custom_objects={'ChannelwiseConv3D': models.ChannelwiseConv3D})\n",
    "            else:\n",
    "                print(\"Warning: incompatible input model type (is not .json nor .h5)\")\n",
    "        else:\n",
    "            params_model = dict(kernel_size=self._kernel_size\n",
    "                            , pool_size=self._pool_size\n",
    "                            , dilation=self._dilation\n",
    "                            , strides=self._strides\n",
    "                            , activation=self._activation\n",
    "                            , padding=self._padding\n",
    "                            , batch_norm=self._batch_norm\n",
    "                            , preproc_layers=self._preproc_layers\n",
    "                            , dropout=self._dropout\n",
    "                            , seed=self._seed\n",
    "                            , growth_rate=self._growth_rate\n",
    "                            , filters=self._filters\n",
    "                            , units=self._units\n",
    "                            , n_encode_layers=self._encode_layers\n",
    "                            , n_regression_layers=self._regression_layers)\n",
    "            if self._unsupervised:\n",
    "                model = models.unsupervised_rigid_concatenated(**params_model)\n",
    "            else:\n",
    "                model = models.rigid_concatenated(**params_model)\n",
    "        return model\n",
    "\n",
    "    def _load_weights(self, model):\n",
    "        if self._weights_dir is not None:\n",
    "            latest_checkpoint = tf.train.latest_checkpoint(self._weights_dir)\n",
    "            model.load_weights(latest_checkpoint)\n",
    "        return model\n",
    "\n",
    "    def _build_data_generators(self):\n",
    "        #TODO: we should use it under preproc\n",
    "        template_filepath = None\n",
    "        unsupervised = False\n",
    "\n",
    "        # if template is undefined, target is the same volume but not moved (motion correction)\n",
    "        if self._use_template:\n",
    "            template_filepath = os.path.join(self._data_dir, \"template_on_grid\")\n",
    "        # if unsupervised, network output is not a transformation but target itself\n",
    "        if self._unsupervised:\n",
    "            unsupervised = True\n",
    "        params_gen = dict(list_files=self._list_files\n",
    "                          , template_file=template_filepath\n",
    "                          , is_unsupervised=unsupervised\n",
    "                          , batch_size=self._batch_size\n",
    "                          , avail_cores=self._ncpu)\n",
    "        self.train_gen = DataGenerator(partition=\"train\", **params_gen)\n",
    "        self.valid_gen = DataGenerator(partition=\"valid\", **params_gen)\n",
    "        self.test_gen = DataGenerator(partition=\"test\", **params_gen)\n",
    "\n",
    "    def create_callbacks(self):\n",
    "        \"\"\"callbacks to optimize lr, tensorboard and checkpoints\"\"\"\n",
    "        model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "            self._ckpt_path, verbose=0, save_weights_only=True, save_freq=\"epoch\")\n",
    "        # reduce_lr_logs = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=1e-10)\n",
    "        tensorboard_dir = os.path.join(self._data_dir\n",
    "                                       , \"../\"\n",
    "                                       , \"tensorboard_logs\"\n",
    "                                       , self._model_name\n",
    "                                       , datetime.datetime.now().strftime(\"%Y/%m/%d/%H:%M:%S\"))\n",
    "        tensorboard_logs = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_dir\n",
    "                                                          , update_freq=\"epoch\"\n",
    "                                                          , histogram_freq=1\n",
    "                                                          , write_graph=False\n",
    "                                                          , write_images=True)\n",
    "        # train_dice_logs = DiceCallback(data_gen=self.train_gen, logs_dir=tensorboard_dir + \"/train_diff\")\n",
    "        # valid_dice_logs = DiceCallback(data_gen=self.valid_gen, logs_dir=tensorboard_dir + \"/validation_diff\")\n",
    "        return [model_ckpt, tensorboard_logs]\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        #configuration for cpu\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(self._ncpu)\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(self._ncpu)\n",
    "\n",
    "        #configuration for gpu\n",
    "        if self._gpu > -1:\n",
    "            os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(self._gpu)\n",
    "            \n",
    "        if self._seed is not None:\n",
    "            os.environ['PYTHONHASHSEED'] = str(self._seed)\n",
    "            rn.seed(self._seed)\n",
    "            np.random.seed(self._seed)\n",
    "            tf.random.set_seed(self._seed)\n",
    "\n",
    "        # generator creation\n",
    "        self._build_data_generators()\n",
    "\n",
    "        # model building\n",
    "        model = self._build_model()\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(lr=self._lr, clipnorm=1., clipvalue=0.5)\n",
    "                      , loss=[metrics.dice_loss if self._unsupervised else metrics.quaternion_mse_loss, metrics.quaternion_penalty])\n",
    "\n",
    "        #by default, if weights_dir is given, the model use them\n",
    "        model = self._load_weights(model)\n",
    "        model.summary(positions=[.30, .65, .80, 1.])\n",
    "        # model.summary(positions=[.30, .70, 1.])\n",
    "        # tf.keras.utils.plot_model(model, show_shapes=True, to_file=os.path.join(self._data_dir, \"../\", \"model.png\")\n",
    "        calls = self.create_callbacks()\n",
    "        calls[-1].set_model(model)\n",
    "\n",
    "        #################### TMP\n",
    "        inputs = self.train_gen.__getitem__(0)[0]\n",
    "        targets = self.train_gen.__getitem__(0)[1]\n",
    "        learning_rate = 5e-4\n",
    "\n",
    "        tf.print(\"### BEFORE WALKING DOWN GRADIENT ###\")\n",
    "        tf.print(\"outputs:\\n\", np.mean(model(inputs)[0]))\n",
    "        tf.print(\"targets:\\n\", np.mean(targets))\n",
    "        tf.print(\"quaternion:\\n\", model(inputs)[1])\n",
    "        tf.print(\"dice:\", metrics.dice_loss(targets, model(inputs)[0]))\n",
    "        tf.print(\"penalty:\", metrics.quaternion_penalty(targets, model(inputs)[1]))\n",
    "\n",
    "        steps = 50  # steps of gradient descent\n",
    "        for s in range(steps):\n",
    "\n",
    "            # ===== Numerical gradient =====\n",
    "            with tf.GradientTape() as t:\n",
    "                current_loss = metrics.dice_loss(targets, model(inputs)[0]) + metrics.quaternion_penalty(targets, model(inputs)[1])\n",
    "\n",
    "            evaluated_gradients = t.gradient(current_loss, model.trainable_weights, unconnected_gradients=tf.UnconnectedGradients.NONE)\n",
    "\n",
    "#                         for i in range(len(model.trainable_weights)):\n",
    "#                             tf.print(\"Layer [%s]\" %(model.get_layer(index=i).name))\n",
    "#                             tf.print(\"grad: \", tf.reduce_mean(evaluated_gradients[i]))\n",
    "#                             tf.print(\"weights: \", tf.reduce_mean(model.trainable_weights[i]))\n",
    "\n",
    "            # Step down the gradient for each layer\n",
    "            for i in range(len(model.trainable_weights)):\n",
    "                model.trainable_weights[i].assign_sub(self._lr * tf.clip_by_value(evaluated_gradients[i], -0.5, 0.5))\n",
    "\n",
    "            # Every 5 steps print the RMSE\n",
    "            if s % 5 == 0:\n",
    "                print(\"### step \" + str(s) + \"###\")\n",
    "#                 tf.print(\"outputs:\\n\", np.mean(model(inputs)[0]))\n",
    "#                 tf.print(\"targets:\\n\", np.mean(targets))\n",
    "#                 tf.print(\"grad:\", evaluated_gradients[-1])\n",
    "                tf.print(\"quaternion:\\n\", model(inputs)[1])\n",
    "                tf.print(\"dice:\", metrics.dice_loss(targets, model(inputs)[0]))\n",
    "                tf.print(\"penalty:\", metrics.quaternion_penalty(targets, model(inputs)[1]))\n",
    "#                             dice = metrics.dice_loss(targets, model(inputs))\n",
    "#                             tf.print(\"dice:\", dice)\n",
    "\n",
    "        tf.print(\"### AFTER STEPPING DOWN GRADIENT ###\")\n",
    "        tf.print(\"outputs:\\n\", np.mean(model(inputs)[0]))\n",
    "        tf.print(\"targets:\\n\", np.mean(targets))\n",
    "        tf.print(\"quaternion:\\n\", model(inputs)[1])\n",
    "        tf.print(\"final dice:\", metrics.dice_loss(targets, model(inputs)[0]))\n",
    "        tf.print(\"final penalty:\", metrics.quaternion_penalty(targets, model(inputs)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Training(batch_size=2\n",
    "                 , data_dir='/DATA/derivatives/deepneuroan/training/generated_data/'\n",
    "                 , encode_layers=5\n",
    "                 , epochs=1500\n",
    "                 , filters=8\n",
    "                 , gpu=1\n",
    "                 , growth_rate=2\n",
    "                 , kernel_size=[5, 5, 5]\n",
    "                 , lr=0.5\n",
    "                 , model_name='unsupervised_kernel_5_lr_005'\n",
    "                 , ncpu=16\n",
    "                 , no_batch_norm=False\n",
    "                 , padding='SAME'\n",
    "                 , pool_size=[2, 2, 2]\n",
    "                 , preproc_layers=1\n",
    "                 , regression_layers=5\n",
    "                 , seed=0\n",
    "                 , strides=[2, 2, 2]\n",
    "                 , units=1024\n",
    "                 , unsupervised=True)\n",
    "train.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/home/ltetrel/notebooks/STN.keras')\n",
    "# sys.path.append('/home/ltetrel/notebooks/STN.keras/src')\n",
    "# sys.path.append('/home/ltetrel/notebooks/STN.keras/src/models_test')\n",
    "\n",
    "# from data_manager import ClutteredMNIST\n",
    "# from visualizer import plot_mnist_sample\n",
    "# from visualizer import print_evaluation\n",
    "# from visualizer import plot_mnist_grid\n",
    "# import tensorflow as tf\n",
    "# import utils_test\n",
    "# import layers_test\n",
    "# import numpy as np\n",
    "\n",
    "# import os\n",
    "# import argparse\n",
    "# import datetime\n",
    "# import platform\n",
    "# import random as rn\n",
    "# sys.path.append('/home/ltetrel/DeepNeuroAN/deepneuroan/')\n",
    "# sys.path.append('/home/ltetrel/DeepNeuroAN/')\n",
    "\n",
    "# from data_generator import DataGenerator\n",
    "# import models\n",
    "# import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"/home/ltetrel/notebooks/STN.keras/datasets/mnist_cluttered_60x60_6distortions.npz\"\n",
    "# batch_size = 256\n",
    "# num_epochs = 30\n",
    "\n",
    "# data_manager = ClutteredMNIST(dataset_path)\n",
    "# train_data, val_data, test_data = data_manager.load()\n",
    "# x_train, y_train = train_data\n",
    "# plot_mnist_sample(x_train[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def STN(input_shape=(60, 60, 1), sampling_size=(30, 30), num_classes=10):\n",
    "#     image = tf.keras.layers.Input(shape=input_shape)\n",
    "#     locnet = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='SAME')(image)\n",
    "#     locnet = tf.keras.layers.Conv2D(20, (5, 5), padding='SAME')(locnet)\n",
    "#     locnet = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='SAME')(locnet)\n",
    "#     locnet = tf.keras.layers.Conv2D(20, (5, 5), padding='SAME')(locnet)\n",
    "#     locnet = tf.keras.layers.Flatten()(locnet)\n",
    "#     locnet = tf.keras.layers.Dense(50)(locnet)\n",
    "#     locnet = tf.keras.layers.Activation('relu')(locnet)\n",
    "#     weights = utils_test.get_initial_weights(50)\n",
    "#     locnet = tf.keras.layers.Dense(6, weights=weights)(locnet)\n",
    "#     x = layers_test.BilinearInterpolation(sampling_size)([image, locnet])\n",
    "#     tf.print(x.shape)\n",
    "#     x = x[:, :, :, 0]\n",
    "#     x = tf.expand_dims(x, axis=-1)\n",
    "#     x = tf.keras.layers.Conv2D(32, (3, 3), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Activation('relu')(x)\n",
    "#     x = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Conv2D(32, (3, 3), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Activation('relu')(x)\n",
    "#     x = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "#     tf.print(x.shape)\n",
    "#     x = tf.keras.layers.Dense(256)(x)\n",
    "#     x = tf.keras.layers.Activation('relu')(x)\n",
    "#     x = tf.keras.layers.Dense(num_classes)(x)\n",
    "#     x = tf.keras.layers.Activation('softmax')(x)\n",
    "#     return tf.keras.models.Model(inputs=image, outputs=x)\n",
    "\n",
    "# model = STN()\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# model.summary()\n",
    "\n",
    "# inputs = tf.random.uniform((4, 60, 60, 1))\n",
    "# targets = [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "#            , [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "#            , [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "#            , [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
    "# learning_rate = 1e-3\n",
    "\n",
    "# print(\"### BEFORE WALKING DOWN GRADIENT ###\")\n",
    "# print(\"outputs:\\n\", model(inputs))\n",
    "# print(\"targets:\\n\", targets)\n",
    "\n",
    "# steps = 100  # steps of gradient descent\n",
    "# for s in range(steps):\n",
    "\n",
    "#     # ===== Numerical gradient =====\n",
    "#     with tf.GradientTape() as t:\n",
    "#         current_loss = tf.keras.losses.categorical_crossentropy(targets, model(inputs))\n",
    "# #                 current_loss = metrics.dice_loss(targets, model(inputs))\n",
    "\n",
    "#     evaluated_gradients = t.gradient(current_loss, model.trainable_weights, unconnected_gradients=tf.UnconnectedGradients.NONE)\n",
    "#     # Step down the gradient for each layer\n",
    "#     for i in range(len(model.trainable_weights)):\n",
    "#         model.trainable_weights[i].assign_sub(1e-3 * evaluated_gradients[i])\n",
    "\n",
    "#     # Every 5 steps print the RMSE\n",
    "#     if s % 10 == 0:\n",
    "#         print(\"### step \" + str(s) + \"###\")\n",
    "# #         for i in range(len(model.trainable_weights)):\n",
    "# #                     print(\"weights:\")\n",
    "# #                     print(tf.reduce_mean(model.trainable_weights[i]))\n",
    "# #             print(\"Layer [%d] grad:\" %i)\n",
    "# #             print(tf.reduce_mean(evaluated_gradients[i])) \n",
    "#         dice = tf.keras.losses.categorical_crossentropy(targets, model(inputs))\n",
    "#         print(\"dice:\", dice)\n",
    "\n",
    "# print(\"### AFTER STEPPING DOWN GRADIENT ###\")\n",
    "# print(\"outputs:\\n\", model(inputs))\n",
    "# print(\"targets:\\n\", targets)\n",
    "# print(\"final dice:\", tf.keras.losses.categorical_crossentropy(targets, model(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def STN(input_shape=(60, 60, 60, 1), sampling_size=(30, 30, 30), num_classes=10):\n",
    "    \n",
    "#     image = tf.keras.layers.Input(shape=input_shape)\n",
    "#     locnet = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='SAME')(image)\n",
    "#     locnet = tf.keras.layers.Conv3D(20, (5, 5, 5), padding='SAME')(locnet)\n",
    "#     locnet = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='SAME')(locnet)\n",
    "#     locnet = tf.keras.layers.Conv3D(20, (5, 5, 5), padding='SAME')(locnet)\n",
    "#     locnet = tf.keras.layers.Flatten()(locnet)\n",
    "#     locnet = tf.keras.layers.Dense(50)(locnet)\n",
    "#     locnet = tf.keras.layers.Activation('relu')(locnet)\n",
    "#     weights = utils_test.get_initial_weights_3D(50)\n",
    "#     locnet = tf.keras.layers.Dense(7, weights=weights)(locnet)\n",
    "#     x = models.LinearTransformation(interp_method=\"nn\", padding_mode=\"zeros\")([image, locnet])\n",
    "#     x = tf.keras.layers.Conv3D(32, (3, 3, 3), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Activation('relu')(x)\n",
    "#     x = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Conv3D(32, (3, 3, 3), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Activation('relu')(x)\n",
    "#     x = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "#     x = tf.keras.layers.Dense(256)(x)\n",
    "#     x = tf.keras.layers.Activation('relu')(x)\n",
    "#     x = tf.keras.layers.Dense(num_classes)(x)\n",
    "#     x = tf.keras.layers.Activation('softmax')(x)\n",
    "#     return tf.keras.models.Model(inputs=image, outputs=x)\n",
    "\n",
    "# model = STN()\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# model.summary()\n",
    "\n",
    "# inputs = tf.random.uniform((4, 60, 60, 60, 1))\n",
    "# targets = [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "#            , [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "#            , [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "#            , [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
    "# learning_rate = 1e-3\n",
    "\n",
    "# print(\"### BEFORE WALKING DOWN GRADIENT ###\")\n",
    "# print(\"outputs:\\n\", model(inputs))\n",
    "# print(\"targets:\\n\", targets)\n",
    "\n",
    "# steps = 25  # steps of gradient descent\n",
    "# for s in range(steps):\n",
    "\n",
    "#     # ===== Numerical gradient =====\n",
    "#     with tf.GradientTape() as t:\n",
    "#         current_loss = tf.keras.losses.categorical_crossentropy(targets, model(inputs))\n",
    "# #                 current_loss = metrics.dice_loss(targets, model(inputs))\n",
    "\n",
    "#     evaluated_gradients = t.gradient(current_loss, model.trainable_weights, unconnected_gradients=tf.UnconnectedGradients.NONE)\n",
    "\n",
    "#     # Every 5 steps print the RMSE\n",
    "#     print(\"### step \" + str(s) + \"###\")\n",
    "#     for i in range(len(model.trainable_weights)):\n",
    "# #                     print(\"weights:\")\n",
    "# #                     print(tf.reduce_mean(model.trainable_weights[i]))\n",
    "#         print(\"Layer [%d] grad:\" %i)\n",
    "# #             print(tf.shape(evaluated_gradients[i]))\n",
    "#         print(np.mean(evaluated_gradients[i]))\n",
    "# #         print(tf.reduce_mean(evaluated_gradients[i])) \n",
    "#     dice = tf.keras.losses.categorical_crossentropy(targets, model(inputs))\n",
    "#     print(\"dice:\", dice)\n",
    "    \n",
    "#         # Step down the gradient for each layer\n",
    "#     for i in range(len(model.trainable_weights)):\n",
    "#         model.trainable_weights[i].assign_sub(1e-3 * evaluated_gradients[i])\n",
    "\n",
    "# print(\"### AFTER STEPPING DOWN GRADIENT ###\")\n",
    "# print(\"outputs:\\n\", model(inputs))\n",
    "# print(\"targets:\\n\", targets)\n",
    "# print(\"final dice:\", tf.keras.losses.categorical_crossentropy(targets, model(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
