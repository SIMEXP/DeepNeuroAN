{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import argparse\n",
    "# import datetime\n",
    "# import platform\n",
    "# import random as rn\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import sys\n",
    "# sys.path.append('/home/ltetrel/DeepNeuroAN/deepneuroan/')\n",
    "# sys.path.append('/home/ltetrel/DeepNeuroAN/')\n",
    "\n",
    "# from data_generator import DataGenerator\n",
    "# import models\n",
    "# import metrics\n",
    "\n",
    "# print(tf.__version__)\n",
    "# print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Training:\n",
    "#     def __init__(self\n",
    "#                  , data_dir=None\n",
    "#                  , ckpt_dir=None\n",
    "#                  , model_path=None\n",
    "#                  , output_model_path=None\n",
    "#                  , model_name=\"rigid_concatenated\"\n",
    "#                  , weights_dir=None\n",
    "#                  , seed=None\n",
    "#                  , epochs=50\n",
    "#                  , batch_size=8\n",
    "#                  , kernel_size=[3, 3, 3]\n",
    "#                  , pool_size=[2, 2, 2]\n",
    "#                  , dilation=[1, 1, 1]\n",
    "#                  , strides=[2, 2, 2]\n",
    "#                  , activation=\"relu\"\n",
    "#                  , padding=\"VALID\"\n",
    "#                  , no_batch_norm=False\n",
    "#                  , preproc_layers=0\n",
    "#                  , motion_correction = False\n",
    "#                  , unsupervised = False\n",
    "#                  , dropout=0\n",
    "#                  , growth_rate=2\n",
    "#                  , filters=4\n",
    "#                  , units=1024\n",
    "#                  , encode_layers=7\n",
    "#                  , regression_layers=4\n",
    "#                  , lr=1e-4\n",
    "#                  , gpu=-1\n",
    "#                  , ncpu=-1):\n",
    "#         self._model_path = model_path\n",
    "#         self._model_name = model_name\n",
    "#         self._weights_dir = weights_dir\n",
    "#         self._epochs = epochs\n",
    "#         self._kernel_size = tuple(kernel_size)\n",
    "#         self._pool_size = tuple(pool_size)\n",
    "#         self._dilation = tuple(dilation)\n",
    "#         self._strides = tuple(strides)\n",
    "#         self._batch_size = int(batch_size)\n",
    "#         self._activation = activation\n",
    "#         self._padding = padding\n",
    "#         self._batch_norm = not no_batch_norm\n",
    "#         self._preproc_layers = preproc_layers\n",
    "#         self._use_template = not motion_correction\n",
    "#         self._unsupervised = unsupervised\n",
    "#         self._dropout = float(dropout)\n",
    "#         self._growth_rate = float(growth_rate)\n",
    "#         self._filters = int(filters)\n",
    "#         self._units = int(units)\n",
    "#         self._encode_layers = int(encode_layers)\n",
    "#         self._regression_layers = int(regression_layers)\n",
    "#         self._lr = lr\n",
    "#         self._gpu = gpu\n",
    "#         self._ncpu = ncpu\n",
    "\n",
    "#         self._data_dir = None\n",
    "#         self._ckpt_dir = None\n",
    "#         self._ckpt_path = None\n",
    "#         self._output_model_path = None\n",
    "#         self._list_files = None\n",
    "#         self._seed = None\n",
    "\n",
    "#         self._set_data_dir(data_dir)\n",
    "#         self._set_seed(seed)\n",
    "#         self._set_list_files()\n",
    "#         self._set_ckpt_dir(ckpt_dir)\n",
    "#         self._set_output_model_path(output_model_path)\n",
    "#         self._set_ncpu()\n",
    "\n",
    "#         self.train_gen = None\n",
    "#         self.valid_gen = None\n",
    "#         self.test_gen = None\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return str(__file__) \\\n",
    "#                + \"\\n\" + str(datetime.datetime.now()) \\\n",
    "#                + \"\\n\" + str(platform.platform()) \\\n",
    "#                + \"\\n\" + \"class Training()\" \\\n",
    "#                + \"\\n\\t input data dir : %s\" % self._data_dir \\\n",
    "#                + \"\\n\\t checkpoint dir : %s\" % self._ckpt_dir \\\n",
    "#                + \"\\n\\t model name : %s\" % self._model_name \\\n",
    "#                + \"\\n\\t weights dir : %s\" % self._weights_dir \\\n",
    "#                + \"\\n\\t seed : %s\" % self._seed \\\n",
    "#                + \"\\n\\t number of epochs : %s\" % (self._epochs,) \\\n",
    "#                + \"\\n\\t batch size : %s\" % self._batch_size \\\n",
    "#                + \"\\n\\t kernel size : %s\" % (self._kernel_size,) \\\n",
    "#                + \"\\n\\t pool size : %s\" % (self._pool_size,) \\\n",
    "#                + \"\\n\\t dilation rate : %s\" % (self._dilation,) \\\n",
    "#                + \"\\n\\t strides : %s\" % (self._strides,) \\\n",
    "#                + \"\\n\\t padding : %s\" % self._padding \\\n",
    "#                + \"\\n\\t activation : %s\" % self._activation \\\n",
    "#                + \"\\n\\t batch norm : %s\" % self._batch_norm \\\n",
    "#                + \"\\n\\t preprocessing (gaussian) layers : %s\" % self._preproc_layers \\\n",
    "#                + \"\\n\\t motion correction : %s\" % (not self._use_template) \\\n",
    "#                + \"\\n\\t unsupervised learning : %s\" % (self._unsupervised) \\\n",
    "#                + \"\\n\\t dropout : %f\" % self._dropout \\\n",
    "#                + \"\\n\\t growth rate : %d\" % self._growth_rate \\\n",
    "#                + \"\\n\\t filters : %d\" % self._filters \\\n",
    "#                + \"\\n\\t units : %d\" % self._units \\\n",
    "#                + \"\\n\\t number of encoding layer : %d\" % self._encode_layers \\\n",
    "#                + \"\\n\\t number of regression layer : %d\" % self._regression_layers \\\n",
    "#                + \"\\n\\t learning rate : %f\" % self._lr \\\n",
    "#                + \"\\n\\t number of cpus : %d\" % self._ncpu \\\n",
    "#                + \"\\n\\t gpu : %d\" % self._gpu\n",
    "\n",
    "#     def _set_data_dir(self, data_dir=None):\n",
    "#         if data_dir is None:\n",
    "#             self._data_dir = os.getcwd()\n",
    "#         else:\n",
    "#             self._data_dir = data_dir\n",
    "\n",
    "#     def _set_ckpt_dir(self, ckpt_dir=None):\n",
    "#         if (ckpt_dir is None) & (self._data_dir is not None):\n",
    "#             self._ckpt_dir = os.path.join(self._data_dir, \"../\", \"checkpoints\")\n",
    "#         else:\n",
    "#             self._ckpt_dir = ckpt_dir\n",
    "#         self._ckpt_path = os.path.join(self._ckpt_dir, \"%s\" % self._model_name, \"%s_cp-{epoch:04d}.ckpt\" % self._model_name)\n",
    "\n",
    "#     def _set_ncpu(self):\n",
    "#         ncpu = self._ncpu\n",
    "#         if ncpu < 0:\n",
    "#             ncpu = os.cpu_count()\n",
    "#         elif ncpu == 0:\n",
    "#             ncpu = 1\n",
    "#         self._ncpu = ncpu\n",
    "\n",
    "#     def _set_output_model_path(self, output_model_path=None):\n",
    "#         if (output_model_path is None) & (self._data_dir is not None):\n",
    "#             self._output_model_path = os.path.join(\n",
    "#                 self._data_dir, \"../\", \"%s_{end_time:s}\" % self._model_name)\n",
    "#         else:\n",
    "#             self._output_model_path = output_model_path\n",
    "\n",
    "#     def _set_seed(self, seed=None):\n",
    "#         if seed is not None:\n",
    "#             self._seed = int(seed)\n",
    "\n",
    "#     def _set_list_files(self):\n",
    "#         self._list_files = []\n",
    "#         list_files_tmp = set([])\n",
    "#         for root, _, files in os.walk(self._data_dir):\n",
    "#             for file in files:\n",
    "#                 filepath = os.path.join(root, file).split('.')[0]\n",
    "#                 if os.path.exists(filepath + \".txt\"):\n",
    "#                     list_files_tmp.add(filepath)\n",
    "#         self._list_files = list(list_files_tmp)\n",
    "#         self._list_files.sort()\n",
    "\n",
    "#     def _build_model(self):\n",
    "#         if self._model_path is not None:\n",
    "#             if self._model_path.split(\".\")[-1] == \"json\":\n",
    "#                 with open(self._model_path, \"r\") as json_file:\n",
    "#                     model = tf.keras.models.model_from_json(json_file.read(), custom_objects={'ChannelwiseConv3D': models.ChannelwiseConv3D})\n",
    "#             elif self._model_path.split(\".\")[-1] == \"h5\":\n",
    "#                 model = tf.keras.models.load_model(self._model_path, custom_objects={'ChannelwiseConv3D': models.ChannelwiseConv3D})\n",
    "#             else:\n",
    "#                 print(\"Warning: incompatible input model type (is not .json nor .h5)\")\n",
    "#         else:\n",
    "#             params_model = dict(kernel_size=self._kernel_size\n",
    "#                             , pool_size=self._pool_size\n",
    "#                             , dilation=self._dilation\n",
    "#                             , strides=self._strides\n",
    "#                             , activation=self._activation\n",
    "#                             , padding=self._padding\n",
    "#                             , batch_norm=self._batch_norm\n",
    "#                             , preproc_layers=self._preproc_layers\n",
    "#                             , dropout=self._dropout\n",
    "#                             , seed=self._seed\n",
    "#                             , growth_rate=self._growth_rate\n",
    "#                             , filters=self._filters\n",
    "#                             , units=self._units\n",
    "#                             , n_encode_layers=self._encode_layers\n",
    "#                             , n_regression_layers=self._regression_layers)\n",
    "#             if self._unsupervised:\n",
    "#                 model = models.unsupervised_rigid_concatenated(**params_model)\n",
    "#             else:\n",
    "#                 model = models.rigid_concatenated(**params_model)\n",
    "#         return model\n",
    "\n",
    "#     def _load_weights(self, model):\n",
    "#         if self._weights_dir is not None:\n",
    "#             latest_checkpoint = tf.train.latest_checkpoint(self._weights_dir)\n",
    "#             model.load_weights(latest_checkpoint)\n",
    "#         return model\n",
    "\n",
    "#     def _build_data_generators(self):\n",
    "#         #TODO: we should use it under preproc\n",
    "#         template_filepath = None\n",
    "#         unsupervised = False\n",
    "\n",
    "#         # if template is undefined, target is the same volume but not moved (motion correction)\n",
    "#         if self._use_template:\n",
    "#             template_filepath = os.path.join(self._data_dir, \"template_on_grid\")\n",
    "#         # if unsupervised, network output is not a transformation but target itself\n",
    "#         if self._unsupervised:\n",
    "#             unsupervised = True\n",
    "#         params_gen = dict(list_files=self._list_files\n",
    "#                           , template_file=template_filepath\n",
    "#                           , is_unsupervised=unsupervised\n",
    "#                           , batch_size=self._batch_size\n",
    "#                           , avail_cores=self._ncpu)\n",
    "#         self.train_gen = DataGenerator(partition=\"train\", **params_gen)\n",
    "#         self.valid_gen = DataGenerator(partition=\"valid\", **params_gen)\n",
    "#         self.test_gen = DataGenerator(partition=\"test\", **params_gen)\n",
    "\n",
    "#     def create_callbacks(self):\n",
    "#         \"\"\"callbacks to optimize lr, tensorboard and checkpoints\"\"\"\n",
    "#         model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "#             self._ckpt_path, verbose=0, save_weights_only=True, save_freq=\"epoch\")\n",
    "#         # reduce_lr_logs = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=1e-10)\n",
    "#         tensorboard_dir = os.path.join(self._data_dir\n",
    "#                                        , \"../\"\n",
    "#                                        , \"tensorboard_logs\"\n",
    "#                                        , self._model_name\n",
    "#                                        , datetime.datetime.now().strftime(\"%Y/%m/%d/%H:%M:%S\"))\n",
    "#         tensorboard_logs = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_dir\n",
    "#                                                           , update_freq=\"epoch\"\n",
    "#                                                           , histogram_freq=1\n",
    "#                                                           , write_graph=False\n",
    "#                                                           , write_images=True)\n",
    "#         # train_dice_logs = DiceCallback(data_gen=self.train_gen, logs_dir=tensorboard_dir + \"/train_diff\")\n",
    "#         # valid_dice_logs = DiceCallback(data_gen=self.valid_gen, logs_dir=tensorboard_dir + \"/validation_diff\")\n",
    "#         return [model_ckpt, tensorboard_logs]\n",
    "\n",
    "#     def add_custom_callbacks(self, calls, train_gen, valid_gen):\n",
    "#         \"\"\"custom callbacks using metrics.py\"\"\"\n",
    "        \n",
    "\n",
    "#     def run(self):\n",
    "\n",
    "#         #configuration for cpu\n",
    "#         tf.config.threading.set_inter_op_parallelism_threads(self._ncpu)\n",
    "#         tf.config.threading.set_intra_op_parallelism_threads(self._ncpu)\n",
    "\n",
    "#         #configuration for gpu\n",
    "#         if self._gpu > -1:\n",
    "#             os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#             os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(self._gpu)\n",
    "#             # physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#             # tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "#         if self._seed is not None:\n",
    "#             os.environ['PYTHONHASHSEED'] = str(self._seed)\n",
    "#             rn.seed(self._seed)\n",
    "#             np.random.seed(self._seed)\n",
    "#             tf.random.set_seed(self._seed)\n",
    "\n",
    "#         # generator creation\n",
    "#         self._build_data_generators()\n",
    "\n",
    "#         # model building\n",
    "#         model = self._build_model()\n",
    "        \n",
    "#         model.compile(optimizer=tf.keras.optimizers.Adam(lr=self._lr)\n",
    "# #                         , loss=[metrics.dice_loss if self._unsupervised else metrics.quaternion_mse_loss]\n",
    "#                         , loss=[\"mae\"])\n",
    "\n",
    "#         #by default, if weights_dir is given, the model use them\n",
    "#         model = self._load_weights(model)\n",
    "#         model.summary(positions=[.30, .65, .80, 1.])\n",
    "#         # model.summary(positions=[.30, .70, 1.])\n",
    "#         # tf.keras.utils.plot_model(model, show_shapes=True, to_file=os.path.join(self._data_dir, \"../\", \"model.png\")\n",
    "#         calls = self.create_callbacks()\n",
    "#         calls[-1].set_model(model)\n",
    "\n",
    "#         #################### TMP\n",
    "#         inputs = tf.random.uniform((4, 220, 220, 220, 2))\n",
    "#         targets = tf.random.uniform((4, 220, 220, 220, 1))\n",
    "#         learning_rate = 1e-3\n",
    "        \n",
    "#         print(\"### BEFORE WALKING DOWN GRADIENT ###\")\n",
    "#         print(\"outputs:\\n\", np.mean(model(inputs)))\n",
    "#         print(\"targets:\\n\", np.mean(targets))\n",
    "\n",
    "#         steps = 25  # steps of gradient descent\n",
    "#         for s in range(steps):\n",
    "\n",
    "#             # ===== Numerical gradient =====\n",
    "#             with tf.GradientTape() as t:\n",
    "#                 current_loss = tf.keras.losses.MAE(targets, model(inputs))\n",
    "# #                 current_loss = metrics.dice_loss(targets, model(inputs))\n",
    "                \n",
    "#             evaluated_gradients = t.gradient(current_loss, model.trainable_weights, unconnected_gradients=tf.UnconnectedGradients.NONE)\n",
    "#             print(evaluated_gradients)\n",
    "#             # Step down the gradient for each layer\n",
    "#             for i in range(len(model.trainable_weights)):\n",
    "#                 model.trainable_weights[i].assign_sub(self._lr * evaluated_gradients[i])\n",
    "            \n",
    "#             # Every 5 steps print the RMSE\n",
    "#             if s % 5 == 0:\n",
    "#                 print(\"### step \" + str(s) + \"###\")\n",
    "#                 for i in range(len(model.trainable_weights)):\n",
    "# #                     print(\"weights:\")\n",
    "# #                     print(tf.reduce_mean(model.trainable_weights[i]))\n",
    "#                     print(\"Layer [%d] grad:\" %i)\n",
    "#                     print(tf.reduce_mean(evaluated_gradients[i])) \n",
    "#                 dice = metrics.dice_loss(targets, model(inputs))\n",
    "#                 print(\"dice:\", dice)\n",
    "\n",
    "#         print(\"### AFTER STEPPING DOWN GRADIENT ###\")\n",
    "#         print(\"outputs:\\n\", np.mean(model(inputs)))\n",
    "#         print(\"targets:\\n\", np.mean(targets))\n",
    "#         print(\"final dice:\", metrics.dice_loss(targets, model(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = Training(batch_size=16\n",
    "#                  , data_dir='/DATA/derivatives/deepneuroan/training/generated_data/'\n",
    "#                  , encode_layers=4\n",
    "#                  , epochs=1500\n",
    "#                  , filters=8\n",
    "#                  , gpu=1\n",
    "#                  , growth_rate=2\n",
    "#                  , kernel_size=[5, 5, 5]\n",
    "#                  , lr=0.5\n",
    "#                  , model_name='unsupervised_kernel_5_lr_005'\n",
    "#                  , ncpu=16\n",
    "#                  , no_batch_norm=False\n",
    "#                  , padding='SAME'\n",
    "#                  , pool_size=[2, 2, 2]\n",
    "#                  , preproc_layers=1\n",
    "#                  , regression_layers=5\n",
    "#                  , seed=0\n",
    "#                  , strides=[2, 2, 2]\n",
    "#                  , units=1024\n",
    "#                  , unsupervised=True)\n",
    "# train.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ltetrel/notebooks/STN.keras')\n",
    "sys.path.append('/home/ltetrel/notebooks/STN.keras/src')\n",
    "sys.path.append('/home/ltetrel/notebooks/STN.keras/src/models_test')\n",
    "\n",
    "from data_manager import ClutteredMNIST\n",
    "from visualizer import plot_mnist_sample\n",
    "from visualizer import print_evaluation\n",
    "from visualizer import plot_mnist_grid\n",
    "import tensorflow as tf\n",
    "import utils_test\n",
    "import layers_test\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import platform\n",
    "import random as rn\n",
    "sys.path.append('/home/ltetrel/DeepNeuroAN/deepneuroan/')\n",
    "sys.path.append('/home/ltetrel/DeepNeuroAN/')\n",
    "\n",
    "from data_generator import DataGenerator\n",
    "import models\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = \"/home/ltetrel/notebooks/STN.keras/datasets/mnist_cluttered_60x60_6distortions.npz\"\n",
    "batch_size = 256\n",
    "num_epochs = 30\n",
    "\n",
    "data_manager = ClutteredMNIST(dataset_path)\n",
    "train_data, val_data, test_data = data_manager.load()\n",
    "x_train, y_train = train_data\n",
    "plot_mnist_sample(x_train[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method BilinearInterpolation.call of <layers_test.BilinearInterpolation object at 0x7fb7265fa828>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method BilinearInterpolation.call of <layers_test.BilinearInterpolation object at 0x7fb7265fa828>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "TensorShape([None, None, None, 2])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The last dimension of the inputs to `Dense` should be defined. Found `None`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d0164f099ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d0164f099ca8>\u001b[0m in \u001b[0;36mSTN\u001b[0;34m(input_shape, sampling_size, num_classes)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2416\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2417\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m       raise ValueError('The last dimension of the inputs to `Dense` '\n\u001b[0m\u001b[1;32m   1155\u001b[0m                        'should be defined. Found `None`.')\n\u001b[1;32m   1156\u001b[0m     \u001b[0mlast_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The last dimension of the inputs to `Dense` should be defined. Found `None`."
     ]
    }
   ],
   "source": [
    "def STN(input_shape=(60, 60, 1), sampling_size=(30, 30), num_classes=10):\n",
    "    image = tf.keras.layers.Input(shape=input_shape)\n",
    "    locnet = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='SAME')(image)\n",
    "    locnet = tf.keras.layers.Conv2D(20, (5, 5), padding='SAME')(locnet)\n",
    "    locnet = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='SAME')(locnet)\n",
    "    locnet = tf.keras.layers.Conv2D(20, (5, 5), padding='SAME')(locnet)\n",
    "    locnet = tf.keras.layers.Flatten()(locnet)\n",
    "    locnet = tf.keras.layers.Dense(50)(locnet)\n",
    "    locnet = tf.keras.layers.Activation('relu')(locnet)\n",
    "    weights = utils_test.get_initial_weights(50)\n",
    "    locnet = tf.keras.layers.Dense(6, weights=weights)(locnet)\n",
    "    x = layers_test.BilinearInterpolation(sampling_size)([image, locnet])\n",
    "    tf.print(x.shape)\n",
    "    x = x[:, :, :, 0]\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), padding='SAME')(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='SAME')(x)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), padding='SAME')(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='SAME')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    tf.print(x.shape)\n",
    "    x = tf.keras.layers.Dense(256)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Dense(num_classes)(x)\n",
    "    x = tf.keras.layers.Activation('softmax')(x)\n",
    "    return tf.keras.models.Model(inputs=image, outputs=x)\n",
    "\n",
    "model = STN()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "inputs = tf.random.uniform((4, 60, 60, 1))\n",
    "targets = [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "           , [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "           , [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "           , [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
    "learning_rate = 1e-3\n",
    "\n",
    "print(\"### BEFORE WALKING DOWN GRADIENT ###\")\n",
    "print(\"outputs:\\n\", model(inputs))\n",
    "print(\"targets:\\n\", targets)\n",
    "\n",
    "steps = 100  # steps of gradient descent\n",
    "for s in range(steps):\n",
    "\n",
    "    # ===== Numerical gradient =====\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = tf.keras.losses.categorical_crossentropy(targets, model(inputs))\n",
    "#                 current_loss = metrics.dice_loss(targets, model(inputs))\n",
    "\n",
    "    evaluated_gradients = t.gradient(current_loss, model.trainable_weights, unconnected_gradients=tf.UnconnectedGradients.NONE)\n",
    "    # Step down the gradient for each layer\n",
    "    for i in range(len(model.trainable_weights)):\n",
    "        model.trainable_weights[i].assign_sub(1e-3 * evaluated_gradients[i])\n",
    "\n",
    "    # Every 5 steps print the RMSE\n",
    "    if s % 10 == 0:\n",
    "        print(\"### step \" + str(s) + \"###\")\n",
    "#         for i in range(len(model.trainable_weights)):\n",
    "#                     print(\"weights:\")\n",
    "#                     print(tf.reduce_mean(model.trainable_weights[i]))\n",
    "#             print(\"Layer [%d] grad:\" %i)\n",
    "#             print(tf.reduce_mean(evaluated_gradients[i])) \n",
    "        dice = tf.keras.losses.categorical_crossentropy(targets, model(inputs))\n",
    "        print(\"dice:\", dice)\n",
    "\n",
    "print(\"### AFTER STEPPING DOWN GRADIENT ###\")\n",
    "print(\"outputs:\\n\", model(inputs))\n",
    "print(\"targets:\\n\", targets)\n",
    "print(\"final dice:\", tf.keras.losses.categorical_crossentropy(targets, model(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def STN(input_shape=(60, 60, 60, 1), sampling_size=(30, 30, 30), num_classes=10):\n",
    "#     image = tf.keras.layers.Input(shape=input_shape)\n",
    "#     locnet = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='SAME')(image)\n",
    "#     locnet = tf.keras.layers.Conv3D(20, (5, 5, 5), padding='SAME')(locnet)\n",
    "#     locnet = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='SAME')(locnet)\n",
    "#     locnet = tf.keras.layers.Conv3D(20, (5, 5, 5), padding='SAME')(locnet)\n",
    "#     locnet = tf.keras.layers.Flatten()(locnet)\n",
    "#     locnet = tf.keras.layers.Dense(50)(locnet)\n",
    "#     locnet = tf.keras.layers.Activation('relu')(locnet)\n",
    "#     weights = utils_test.get_initial_weights_3D(50)\n",
    "#     locnet = tf.keras.layers.Dense(7, weights=weights)(locnet)\n",
    "#     x = models.LinearTransformation(sampling_size)([image, locnet])\n",
    "#     x = tf.keras.layers.Conv3D(32, (3, 3, 3), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Activation('relu')(x)\n",
    "#     x = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Conv3D(32, (3, 3, 3), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Activation('relu')(x)\n",
    "#     x = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), padding='SAME')(x)\n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "#     x = tf.keras.layers.Dense(256)(x)\n",
    "#     x = tf.keras.layers.Activation('relu')(x)\n",
    "#     x = tf.keras.layers.Dense(num_classes)(x)\n",
    "#     x = tf.keras.layers.Activation('softmax')(x)\n",
    "#     return tf.keras.models.Model(inputs=image, outputs=x)\n",
    "\n",
    "# model = STN()\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# model.summary()\n",
    "\n",
    "# inputs = tf.random.uniform((4, 60, 60, 60, 1))\n",
    "# targets = [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "#            , [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "#            , [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "#            , [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
    "# learning_rate = 1e-3\n",
    "\n",
    "# print(\"### BEFORE WALKING DOWN GRADIENT ###\")\n",
    "# print(\"outputs:\\n\", model(inputs))\n",
    "# print(\"targets:\\n\", targets)\n",
    "\n",
    "# steps = 25  # steps of gradient descent\n",
    "# for s in range(steps):\n",
    "\n",
    "#     # ===== Numerical gradient =====\n",
    "#     with tf.GradientTape() as t:\n",
    "#         current_loss = tf.keras.losses.categorical_crossentropy(targets, model(inputs))\n",
    "# #                 current_loss = metrics.dice_loss(targets, model(inputs))\n",
    "\n",
    "#     evaluated_gradients = t.gradient(current_loss, model.trainable_weights, unconnected_gradients=tf.UnconnectedGradients.NONE)\n",
    "\n",
    "#     # Every 5 steps print the RMSE\n",
    "#     print(\"### step \" + str(s) + \"###\")\n",
    "#     for i in range(len(model.trainable_weights)):\n",
    "# #                     print(\"weights:\")\n",
    "# #                     print(tf.reduce_mean(model.trainable_weights[i]))\n",
    "#         print(\"Layer [%d] grad:\" %i)\n",
    "# #             print(tf.shape(evaluated_gradients[i]))\n",
    "#         print(evaluated_gradients[i]) \n",
    "# #         print(tf.reduce_mean(evaluated_gradients[i])) \n",
    "#     dice = tf.keras.losses.categorical_crossentropy(targets, model(inputs))\n",
    "#     print(\"dice:\", dice)\n",
    "    \n",
    "#         # Step down the gradient for each layer\n",
    "#     for i in range(len(model.trainable_weights)):\n",
    "#         model.trainable_weights[i].assign_sub(1e-3 * evaluated_gradients[i])\n",
    "\n",
    "# print(\"### AFTER STEPPING DOWN GRADIENT ###\")\n",
    "# print(\"outputs:\\n\", model(inputs))\n",
    "# print(\"targets:\\n\", targets)\n",
    "# print(\"final dice:\", tf.keras.losses.categorical_crossentropy(targets, model(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
